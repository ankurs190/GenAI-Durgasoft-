{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K30XKrq5Ig3G",
        "rhUbyCjtmPuh",
        "Z95ZEX-8vTZn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankurs190/GenAI-Durgasoft-/blob/main/NLP_Durgasoft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **NLP Summary**"
      ],
      "metadata": {
        "id": "pZ-uFyW75y5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Step                    | Description                                       | Tool/Library                | Example Input                                   | Example Output             |\n",
        "| ----------------------- | ------------------------------------------------- | --------------------------- | ----------------------------------------------- | -------------------------- |\n",
        "| **Lowercasing**         | Converts all text to lowercase                    | Python string methods       | \"Hello World\"                                   | \"hello world\"              |\n",
        "| **Tokenization**        | Splits text into words/sentences                  | `nltk.word_tokenize()`      | \"I love NLP.\"                                   | \\['I', 'love', 'NLP', '.'] |\n",
        "| **Stopword Removal**    | Removes common words with little meaning          | `nltk.corpus.stopwords`     | \"I am learning NLP\"                             | \\['learning', 'NLP']       |\n",
        "| **Punctuation Removal** | Removes symbols like `.,!?`                       | `string.punctuation` + `re` | \"Hello!\"                                        | \"Hello\"                    |\n",
        "| **Stemming**            | Trims words to their root form                    | `PorterStemmer`, `Snowball` | \"running\", \"flies\"                              | \"run\", \"fli\"               |\n",
        "| **Lemmatization**       | Converts word to dictionary base form using POS   | `WordNetLemmatizer`         | \"better\", \"was\"                                 | \"good\", \"be\"               |\n",
        "| **POS Tagging**         | Tags words with part of speech (noun, verb, etc.) | `nltk.pos_tag()`            | \"run\", \"beautiful\"                              | \\[('run', 'VB'), ...]      |\n",
        "| **Regex Cleaning**      | Custom text cleanup using patterns                | `re` module                 | \"[User123@gmail.com](mailto:User123@gmail.com)\" | \"User\" (after regex)       |\n",
        "\n",
        "\n",
        "- NLP Text Hierarchy- Corpus-> Document-> Sentence-> Vocabulary\n",
        "\n",
        "| Level          | Description                                                                         | Example                            |\n",
        "| -------------- | ----------------------------------------------------------------------------------- | ---------------------------------- |\n",
        "| **Corpus**     | A **collection of documents**. The entire dataset you're analyzing.                 | All the articles in a news archive |\n",
        "| **Document**   | A **single piece of text** (can be an article, paragraph, review, etc.)             | One news article                   |\n",
        "| **Sentence**   | A **meaningful sequence of words**, typically ending in a period/question mark/etc. | \"The cat sat on the mat.\"          |\n",
        "| **Vocabulary** | The set of **unique words** in the corpus (after cleaning).                         | {“cat”, “sat”, “mat”, ...}         |\n"
      ],
      "metadata": {
        "id": "QzNtCHdn5t_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 69 (25-June)**"
      ],
      "metadata": {
        "id": "GcRklRwsz3bP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **String Punctuation**\n"
      ],
      "metadata": {
        "id": "K30XKrq5Ig3G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "DJ7pADKSsOBl"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oFQupiCyscoK",
        "outputId": "21ef20ab-64d6-425f-de3a-e994cee30c89"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punc = string.punctuation"
      ],
      "metadata": {
        "id": "9coT-5IfslkK"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NI8nyBwDsu84",
        "outputId": "26c00e67-7851-4aff-8da5-2b1d9d9a26ee"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg = \"Good Evening Everyone, Welcome to 'NLP' Class (AI)\""
      ],
      "metadata": {
        "id": "1l8TCLCDsvj-"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9pPPa_2QtLlO",
        "outputId": "60a2e222-60be-4ea9-99ee-55e681d56ff5"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Good Evening Everyone, Welcome to 'NLP' Class (AI)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "pTLPxJbst1GP"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in msg :\n",
        "  print(i)\n",
        "  time.sleep(0.5)  # prints each character (or item) in msg, one at a time, with a 0.5 second delay between each — creating a \"typing effect\"."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1CSJEuftdRm",
        "outputId": "b46ee9d6-819e-4061-8c97-e6b44ac61b08",
        "collapsed": true
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\n",
            "o\n",
            "o\n",
            "d\n",
            " \n",
            "E\n",
            "v\n",
            "e\n",
            "n\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "E\n",
            "v\n",
            "e\n",
            "r\n",
            "y\n",
            "o\n",
            "n\n",
            "e\n",
            ",\n",
            " \n",
            "W\n",
            "e\n",
            "l\n",
            "c\n",
            "o\n",
            "m\n",
            "e\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "'\n",
            "N\n",
            "L\n",
            "P\n",
            "'\n",
            " \n",
            "C\n",
            "l\n",
            "a\n",
            "s\n",
            "s\n",
            " \n",
            "(\n",
            "A\n",
            "I\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eHO-JRGeuM51",
        "outputId": "07cf22a3-be3e-414b-b03a-2e0b283cb71a"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5LqPAOBIuUjJ",
        "outputId": "aa5fffc0-2ab4-4dd8-a8b6-dab06af63f58"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Good Evening Everyone, Welcome to 'NLP' Class (AI)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in msg :\n",
        "   if i not in punc:\n",
        "      print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miUPpZFGtucr",
        "outputId": "4a87a4e2-266c-42b9-c782-03ac8dedac64",
        "collapsed": true
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\n",
            "o\n",
            "o\n",
            "d\n",
            " \n",
            "E\n",
            "v\n",
            "e\n",
            "n\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "E\n",
            "v\n",
            "e\n",
            "r\n",
            "y\n",
            "o\n",
            "n\n",
            "e\n",
            " \n",
            "W\n",
            "e\n",
            "l\n",
            "c\n",
            "o\n",
            "m\n",
            "e\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "N\n",
            "L\n",
            "P\n",
            " \n",
            "C\n",
            "l\n",
            "a\n",
            "s\n",
            "s\n",
            " \n",
            "A\n",
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slist = []\n",
        "for i in msg :\n",
        "   if i not in punc:\n",
        "      slist.append(i)\n",
        "print(slist)\n",
        "print(\"\".join(slist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6teWf7guTeq",
        "outputId": "9b275395-a527-479a-de03-515ed06e9cec"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['G', 'o', 'o', 'd', ' ', 'E', 'v', 'e', 'n', 'i', 'n', 'g', ' ', 'E', 'v', 'e', 'r', 'y', 'o', 'n', 'e', ' ', 'W', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'N', 'L', 'P', ' ', 'C', 'l', 'a', 's', 's', ' ', 'A', 'I']\n",
            "Good Evening Everyone Welcome to NLP Class AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join([ c    for c in msg  if c not in punc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "F7TqHDqduqrP",
        "outputId": "10377528-910a-4631-ed1f-85c9ca90456b"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Good Evening Everyone Welcome to NLP Class AI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 70 (25 June)**"
      ],
      "metadata": {
        "id": "rhUbyCjtmPuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Special Character with Regular Expression**"
      ],
      "metadata": {
        "id": "Z95ZEX-8vTZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wish = \"Wish -You -A@ Happy_ New_ Year #2025!\""
      ],
      "metadata": {
        "id": "hHguVisDvFW6"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wish"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XTNHKbD3xBJE",
        "outputId": "bb5c611c-8284-4ac8-8599-3d76ebabd12c"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Wish -You -A@ Happy_ New_ Year #2025!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "r4_V0rDCxGcO"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[a-z]\", wish)  # uses Python’s re module to find all lowercase alphabetic characters (a to z) in the string wish."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlAd5g2lxBpp",
        "outputId": "5c346cb3-cc47-4efb-8a25-8f54365902a5"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 's', 'h', 'o', 'u', 'a', 'p', 'p', 'y', 'e', 'w', 'e', 'a', 'r']"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re.findall(\"[a-zA-Z]\", wish)"
      ],
      "metadata": {
        "id": "Qq5a008bxj1-"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[a-zA-Z0-9]\", wish)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L2IGAQwxoXp",
        "outputId": "cb6e6482-07ad-451e-9064-887346039f10"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['W',\n",
              " 'i',\n",
              " 's',\n",
              " 'h',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'u',\n",
              " 'A',\n",
              " 'H',\n",
              " 'a',\n",
              " 'p',\n",
              " 'p',\n",
              " 'y',\n",
              " 'N',\n",
              " 'e',\n",
              " 'w',\n",
              " 'Y',\n",
              " 'e',\n",
              " 'a',\n",
              " 'r',\n",
              " '2',\n",
              " '0',\n",
              " '2',\n",
              " '5']"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[^a-zA-Z0-9 ]\", wish)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf3_uxvExyHN",
        "outputId": "1363e77a-1cb9-4479-edd6-2d508c4cc023"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-', '-', '@', '_', '_', '#', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub(\"[^a-zA-Z0-9 ]\",\"\", wish)  #removes all characters from the string wish that are not letters, digits, or spaces."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IY_bI0nhx7aB",
        "outputId": "b2bb8cb4-d838-4c11-faad-786a245aa80c"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Wish You A Happy New Year 2025'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re.findall()\t#Returns all matches in a list\n",
        "# re.search()\t#Returns first match (or None)\n",
        "# re.match()\t#Matches only from start of string\n",
        "# re.sub()\t#Substitutes matches with a new string\n",
        "# re.split()\t#Splits string based on a pattern\n",
        "# re.compile()\t#Precompiles a regex pattern for reuse"
      ],
      "metadata": {
        "id": "iVbNDx1uA16d"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Tokenization**\n",
        "\n",
        "* Tokenization is the process of breaking text into smaller pieces called tokens. These tokens can be:\n",
        " * Words, Subwords, Characters, or even punctuation marks\n",
        "* It’s a fundamental step in NLP used by models like GPT, BERT, etc.\n",
        "\n",
        "* nltk, spaCy, and HuggingFace tokenizers for both sentence and word tokenization"
      ],
      "metadata": {
        "id": "YjJQ-b20yb4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install nltk\n",
        "import nltk  #  Natural Language Toolkit\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')  #downloads the Punkt tokenizer models for the NLTK.This is required for functions like word_tokenize() to work properly.\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzTcVmdIyQG1",
        "outputId": "9fe12d36-f670-46de-8a8d-a0a3e610f3ff"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"Hello All! How are you doing today? We are implementing tokenization in NLP. This is done using NLTK.\"\n",
        "print(word_tokenize(para))\n",
        "# splits punctuation as separate tokens. Notice how !, ?, and . are treated as separate tokens\n",
        "# This is ideal for tasks like POS tagging, NER, or training models\n",
        "\n",
        "\n",
        "print(sent_tokenize(para)) #  keeps punctuation attached to sentence\n",
        "# Sentence boundaries are inferred based on punctuation and capitalization patterns"
      ],
      "metadata": {
        "id": "FOsXM0I3zu-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b3aad1-cd1d-4b3a-f3f1-5ff09cd20c0e"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'All', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'We', 'are', 'implementing', 'tokenization', 'in', 'NLP', '.', 'This', 'is', 'done', 'using', 'NLTK', '.']\n",
            "['Hello All!', 'How are you doing today?', 'We are implementing tokenization in NLP.', 'This is done using NLTK.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(para.split())  #Splits text only on whitespace. Does not separate punctuation from words\n",
        "print(word_tokenize(para))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtnXL43l1-1A",
        "outputId": "f76407ae-7080-4c5a-e78e-a44579fdc883"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'All!', 'How', 'are', 'you', 'doing', 'today?', 'We', 'are', 'implementing', 'tokenization', 'in', 'NLP.', 'This', 'is', 'done', 'using', 'NLTK.']\n",
            "['Hello', 'All', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'We', 'are', 'implementing', 'tokenization', 'in', 'NLP', '.', 'This', 'is', 'done', 'using', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Stop Words**"
      ],
      "metadata": {
        "id": "d8puZ7ehIGJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Stop words are common words in a language (like \"the\", \"is\", \"in\", \"and\") that are usually filtered out before processing text in NLP, because they don’t carry much meaningful information. Doesn't contribute to semantic meaning of seuqential data\n",
        "\n",
        "* Why Remove Stop Words?\n",
        "   * They appear very frequently, but add little value to tasks like:\n",
        "    * Text classification\n",
        "    * Topic modeling\n",
        "    * Information retrieval\n",
        "   * Removing them reduces noise (focus on important words) and sometimes improves performance (by removing words, reduced memory usage)\n",
        "\n",
        "* when to remove stopwords-\n",
        "  * Text Classification -\n",
        "    * spam detection: if mail is having words like-\n",
        "      * Discount, offer, personal loans, excellent opportunity\n",
        "  * Search Engine- To ignore irrelevant terms in queries\n",
        "  * Document Clustering\n",
        "  * Sentiment Analysis\n",
        "  * Topic modeling (LDA): Stopwords add noise\n",
        "  * TF-IDF vectorization: Stopwords often have high frequency but low informativeness\n",
        "\n",
        "\n",
        "\n",
        "* When You Don’t Remove Them\n",
        "   * **Context-sensitive models (BERT, GPT)**: These models learn to assign low weight to stopwords, so removal isn't necessary.\n",
        "   * **Sequence tasks (e.g., translation, summarization)**: Removing stopwords would distort grammar and meaning."
      ],
      "metadata": {
        "id": "fu7CFqTvGK2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence: \"The cat is on the mat.\"\n",
        "\n",
        "# Without stopwords: \"cat mat\"\n",
        "\n",
        "# For a classifier, this may retain core meaning\n",
        "\n",
        "# For a translator or summarizer — this would break the sentence\n"
      ],
      "metadata": {
        "id": "2toSEx8b2hqp"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  # imports the stop words corpus from the NLTK library, which provides predefined lists of common stop words in multiple languages (like English, French, German, etc.).\n",
        "nltk.download('stopwords') # Download the stopwords data (only once):\n",
        "print(stopwords.words('english'))\n",
        "print(len(stopwords.words('english')))"
      ],
      "metadata": {
        "id": "npW6qRCf2QNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7540e312-7b6e-4906-c79c-5b14550b6450"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
            "198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To list available languages like:\n",
        "print(stopwords.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJbvMKfgGjRX",
        "outputId": "76e1667b-b440-4e95-9e3a-f4f0501061e8"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['albanian', 'arabic', 'azerbaijani', 'basque', 'belarusian', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'tamil', 'turkish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filtered\n",
        "print([word for word in word_tokenize(para) if word.lower() not in stopwords.words('english') and word not in string.punctuation])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZuVBar7HG7d",
        "outputId": "ecd53dd6-b939-4cad-8fde-50de7b53a1c0"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'today', 'implementing', 'tokenization', 'NLP', 'done', 'using', 'NLTK']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 71 - (26 June)**"
      ],
      "metadata": {
        "id": "2dsUXDmX0OjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = \"\"\"Defence Minister Rajnath Singh on Thursday has reportedly refused to sign the joint statement of the\n",
        "Shanghai Cooperation Organisation (SCO). As per reports, the defence minister refused to dot India's name on the\n",
        "document due to its failure to address India's concern regarding cross-border terrorism.\"\"\""
      ],
      "metadata": {
        "id": "5LlMUD8mHlS8"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(news)"
      ],
      "metadata": {
        "id": "Hb2cc2n_0RDP"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words:\n",
        "  if w not in stopwords.words('english'):\n",
        "    print(w,end=' ')\n",
        "    time.sleep(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifdhnSgVpFaA",
        "outputId": "9b516298-bed1-425d-8f42-6e1d4cb1ecba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defence Minister Rajnath Singh "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join([w for w in words if w not in stopwords.words('english')])"
      ],
      "metadata": {
        "id": "3_nIfxzypcFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Stemming**"
      ],
      "metadata": {
        "id": "zDaY1dhxskRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
      ],
      "metadata": {
        "id": "ghnxVsXcqf9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps= PorterStemmer()  #One of the oldest and most common stemmers.Conservative: trims suffixes but preserves meaning better.\n",
        "ss=SnowballStemmer(language = \"english\") # More modern, multilingual, and slightly more aggressive than Porter. Often preferred over Porter for non-English or consistent stemming.\n",
        "ls=LancasterStemmer() #Very aggressive. May cut too much and produce root forms that aren't real words."
      ],
      "metadata": {
        "id": "pHofwlBOqzlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem(\"running\")  # Output: run\n",
        "ps.stem(\"studies\")  # Output: studi\n",
        "ps.stem(\"happiness\")  # Output: happi\n",
        "ps.stem(\"maximum\")  # Output: maximum\n",
        "ps.stem(\"writing\") # write\n",
        "\n",
        "ss.stem(\"running\")  # Output: run\n",
        "ss.stem(\"studies\")  # Output: happi\n",
        "ss.stem(\"happiness\")  # Output: happy\n",
        "ss.stem(\"maximum\")  # Output: maximum\n",
        "ss.stem(\"writing\") # write\n",
        "\n",
        "ls.stem(\"running\")  # Output: run\n",
        "ls.stem(\"studies\")  # Output: study\n",
        "ls.stem(\"happiness\")  # Output: happy\n",
        "ls.stem(\"maximum\")  # Output: maxim\n",
        "ls.stem(\"writing\") # writ\n",
        "\n",
        "# Each stemmer is useful in different contexts:\n",
        "  # Use Porter for general English.\n",
        "  # Use Snowball for multilingual or slightly better balance.\n",
        "  # Use Lancaster when you need shorter roots but can tolerate distortion."
      ],
      "metadata": {
        "id": "HjwkwGd62zBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ps.stem(\"writing\"))\n",
        "print(ss.stem(\"writing\"))\n",
        "print(ls.stem(\"writing\"))"
      ],
      "metadata": {
        "id": "V2L2etbbq-Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inflected_words = [\"code\", \"coder\", \"coding\", \"coders\", \"codings\", \"change\", \"changes\", \"changing\", \"changed\",\n",
        "                   \"trouble\", \"troubled\",\"troubling\", \"troubles\", \"university\", \"universities\",\"universe\", \"universal\", \"run\",\n",
        "                   \"ran\", \"running\", \"runs\", \"writer\", \"write\", \"writers\", \"writes\", \"writing\"]\n",
        "print([ps.stem(i) for i in inflected_words])\n",
        "print([ss.stem(i) for i in inflected_words])\n",
        "print([ls.stem(i) for i in inflected_words])"
      ],
      "metadata": {
        "id": "zc9WOlCirWsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for  word in inflected_words:\n",
        "#   p = ps.stem(word)\n",
        "#   s = ss.stem(word)\n",
        "#   l = ls.stem(word)\n",
        "#   print(word,\"===>\",p,\"===>\",s,\"===>\", l)"
      ],
      "metadata": {
        "id": "CUAP-N6VsHYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Lemmatization**\n",
        "- Lemmatization is a NLP technique used to reduce words to their base or dictionary form, known as a lemma.\n",
        "- Unlike stemming, which often chops off word endings without understanding context, lemmatization uses vocabulary and grammar rules to return real words.\n",
        "- Lemmatizers consider the part of speech (POS) to determine the correct lemma. For example:\n",
        "  - running as a verb → run\n",
        "  - running as a noun (e.g., \"the running was fun\") → remains running"
      ],
      "metadata": {
        "id": "kZIU1GztuSIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "lemma = nltk.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "9VmHvSj8sbBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize(\"writing\")\n",
        "# running\trun\n",
        "# better\tgood\n",
        "# studies\tstudy\n",
        "# was\tbe"
      ],
      "metadata": {
        "id": "u3naoFZMsyg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inflected_words = [\"code\", \"coder\", \"coding\", \"coders\", \"codings\", \"change\", \"changes\", \"changing\", \"changed\",\n",
        "                   \"trouble\", \"troubled\",\"troubling\", \"troubles\", \"university\", \"universities\",\"universe\", \"universal\", \"run\",\n",
        "                   \"ran\", \"running\", \"runs\", \"writer\", \"write\", \"writers\", \"writes\", \"writing\"]\n",
        "print([lemma.lemmatize(i) for i in inflected_words])"
      ],
      "metadata": {
        "id": "okM3Oib_s86w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for w in inflected_words:\n",
        "#   res = lemma.lemmatize(w)\n",
        "#   print(w,\" ===> \",res)"
      ],
      "metadata": {
        "id": "_HzXsvYFszB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer  # The main lemmatizer class in NLTK.\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.corpus import wordnet  # provides part-of-speech (POS) constants (like wordnet.NOUN).\n",
        "from nltk import pos_tag, word_tokenize  # pos_tag tags each word in a sentence with its part of speech (e.g., noun, verb)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()  # Creates an instance of the lemmatizer.\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character for WordNetLemmatizer.\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {'J': wordnet.ADJ,\n",
        "                'N': wordnet.NOUN,\n",
        "                'V': wordnet.VERB,\n",
        "                'R': wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "word = \"writing\"\n",
        "lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "print(lemmatized_word)  # Output: run\n"
      ],
      "metadata": {
        "id": "lan9VvOws2wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = \"\"\"Defence Minister Rajnath Singh on Thursday has reportedly refused to sign the joint statement of the\n",
        "Shanghai Cooperation Organisation (SCO). 123456 As per reports, the defence minister refused to dot India's name on the\n",
        "document due to its failure to address India's concern regarding cross-border terrorism.\"\"\""
      ],
      "metadata": {
        "id": "TLKfKsomvd1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "''.join([i for i in news if  i not in string.punctuation])"
      ],
      "metadata": {
        "id": "S1fV3EeJwX8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# string.punctuation  #-> Contains all punctuation characters:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~. Used to remove punctuation during text cleaning.\n",
        "# re  #Python's built-in module for pattern matching and text manipulation. Common use: remove unwanted patterns like extra spaces, special characters, digits.\n",
        "# Tokenizations # Splits text into smaller units: words (word tokenization) or sentences (sentence tokenization).\n",
        "# Stopswords removal -> Stopwords are common words (e.g., “the”, “is”, “and”) that usually carry less meaning.\n",
        "# Stemming  -> Reduces words to their root form, which may not always be a valid word.\n",
        "# lemmatization -> Similar to stemming, but returns the dictionary base form (lemma) and uses POS tagging."
      ],
      "metadata": {
        "id": "pPnubO7ZwZ3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature              | `stopwords`                          | `string.punctuation`                  | `re` (Regular Expressions)                |                                            |\n",
        "| -------------------- | ------------------------------------ | ------------------------------------- | ----------------------------------------- | ------------------------------------------ |\n",
        "| **Module**           | `nltk.corpus.stopwords`              | Python `string` module                | Python `re` module                        |                                            |\n",
        "| **Purpose**          | Remove common, low-value words       | Remove punctuation/special characters | Remove or match patterns (flexible)       |                                            |\n",
        "| **Example Elements** | \"the\", \"is\", \"and\", \"in\", \"at\"       | `!\"#$%&'()*+,-./:;<=>?@[\\]^_`{        | }\\~\\`                                     | Any text pattern (e.g., `\\d+`, `[A-Za-z]`) |\n",
        "| **Use Case**         | Text simplification, keyword focus   | Cleaning symbols from text            | Cleaning, pattern matching, substitutions |                                            |\n",
        "| **Customizable?**    | Yes (you can add/remove words)       | No (fixed punctuation list)           | Yes (very flexible patterns)              |                                            |\n",
        "| **Example Code**     | `word in stopwords.words('english')` | `if char in string.punctuation`       | `re.sub(r'\\d+', '', text)`                |                                            |\n",
        "| **Returns**          | List of stop words                   | String of punctuation characters      | Modified string or match object           |                                            |\n",
        "\n",
        "-----------------------\n",
        "\n",
        "| Feature | **Stemming**                                | **Lemmatization**                                 |\n",
        "| ------- | ------------------------------------------- | ------------------------------------------------- |\n",
        "| Purpose | Reduces words to **root forms** by trimming | Reduces words to **dictionary base form** (lemma) |\n",
        "| Output  | May not be a real word                      | Always produces a valid word                      |\n",
        "| Method  | Rule-based, mechanical                      | Rule-based **+ vocabulary + grammar (POS)**       |\n",
        "\n",
        "| Scenario                                                   | Choose                                                |\n",
        "| ---------------------------------------------------------- | ----------------------------------------------------- |\n",
        "| Speed-sensitive pipeline                                   | Stemming                                              |\n",
        "| Meaning-preserving NLP task (like QA, chatbots, sentiment) | Lemmatization                                         |\n",
        "| Indexing/search engines                                    | Stemming (e.g., \"run\", \"runs\", \"running\" all → \"run\") |\n",
        "\n",
        "| Word             | **PorterStemmer** | **SnowballStemmer** | **LancasterStemmer** | **Lemmatization** |\n",
        "| ---------------- | ----------------- | ------------------- | -------------------- | ----------------- |\n",
        "| **running**      | run               | run                 | run                  | run               |\n",
        "| **studies**      | studi             | studi               | study                | study             |\n",
        "| **flies**        | fli               | fli                 | fly                  | fly               |\n",
        "| **happily**      | happili           | happili             | happy                | happily           |\n",
        "| **better**       | better            | better              | bet                  | good *(adj)*      |\n",
        "| **caring**       | care              | care                | car                  | care              |\n",
        "| **maximum**      | maximum           | maximum             | maxim                | maximum           |\n",
        "| **organization** | organ             | organ               | organ                | organization      |\n",
        "| **was**          | wa                | wa                  | was                  | be *(verb)*       |\n",
        "| **children**     | children          | children            | child                | child             |\n",
        "\n"
      ],
      "metadata": {
        "id": "yuEIBCzP4rEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# print(stopwords.words('english'))\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer\n",
        "ps= PorterStemmer()\n",
        "ss=SnowballStemmer(language = \"english\")\n",
        "ls=LancasterStemmer()\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemma = nltk.WordNetLemmatizer()\n",
        "\n"
      ],
      "metadata": {
        "id": "34M3ObBRxoo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join([lemma.lemmatize(ps.stem(i)) for i in word_tokenize (news) if i not in stopwords.words('english') and i not in string.punctuation])\n",
        "# ' '.join([ps.stem(i) for i in word_tokenize (news) if i not in stopwords.words('english') and i not in string.punctuation])"
      ],
      "metadata": {
        "id": "70rs9TWgx0zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join([lemma.lemmatize(i) for i in word_tokenize (news) if i not in stopwords.words('english') and i not in string.punctuation])"
      ],
      "metadata": {
        "id": "zBgFR7FBy09e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full pipeline example\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download required resources (only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Input text\n",
        "text = \"Running faster than the wind, she was better at it than anyone!\"\n",
        "\n",
        "# Step 1: Lowercasing\n",
        "text = text.lower()\n",
        "\n",
        "# Step 2: Remove punctuation using regex\n",
        "text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Step 4: Stopword removal\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "# Step 5: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(w) for w in filtered_tokens]\n",
        "\n",
        "# Step 6: Lemmatization with POS\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    pos_dict = {'J': wordnet.ADJ, 'N': wordnet.NOUN,\n",
        "                'V': wordnet.VERB, 'R': wordnet.ADV}\n",
        "    return pos_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in filtered_tokens]\n",
        "\n",
        "# Output\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"After Stopword Removal:\", filtered_tokens)\n",
        "print(\"After Stemming:\", stemmed)\n",
        "print(\"After Lemmatization:\", lemmatized)\n"
      ],
      "metadata": {
        "id": "nL_26o_e0CvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 72- NLP Pipeline - (27 June)**"
      ],
      "metadata": {
        "id": "EzWmjFJ47zp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "df = pd.read_csv(\"/content/IMDB.csv\", on_bad_lines='skip')\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "Y4RPvyU61OvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.sample(10000)\n",
        "df1[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "id": "_PuiOL-P-Wqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.reset_index(inplace = True)"
      ],
      "metadata": {
        "id": "KXIQSzAf-l3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head(2)"
      ],
      "metadata": {
        "id": "TfTXRAVZ-xQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.drop(\"index\", axis = 1, inplace=True)"
      ],
      "metadata": {
        "id": "G-7GXHXX-1XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.review.head(3)"
      ],
      "metadata": {
        "id": "ERr50LKT_API"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df1[\"review\"].map(lambda x : x.upper())\n",
        "# df1[\"review\"].map(lambda x : x.lower())\n",
        "\n",
        "df1[\"Result\"] = df1[\"review\"].map(lambda x : x.lower())\n",
        "\n",
        "# df1[\"Result\"].map(lambda x :  re.findall(\"<.*?>\", x))\n",
        "# df1[\"Result\"].map(lambda x :  len(x))   # length before\n",
        "# df1[\"Result\"].map(lambda x :  re.sub(\"<.*?>\",\"\", x))\n",
        "\n",
        "df1[\"Result\"] = df1[\"Result\"].map(lambda x :  re.sub(\"<.*?>\",\"\", x))\n",
        "\n",
        "# df1[\"Result\"].map(lambda x :  len(x))  #length after"
      ],
      "metadata": {
        "id": "97QpFPlM-5Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from string import punctuation\n",
        "punc = punctuation"
      ],
      "metadata": {
        "id": "57QDYRQ8--tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"Result\"][0]"
      ],
      "metadata": {
        "id": "u5GV44Tp_r7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join( [ c   for c in df1[\"Result\"][0] if c not in punc] )"
      ],
      "metadata": {
        "id": "tCkBk2VBCD4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df1[\"Result\"][0])"
      ],
      "metadata": {
        "id": "DQWDpCSWCGRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(\"\".join( [ c   for c in df1[\"Result\"][0] if c not in punc] ))"
      ],
      "metadata": {
        "id": "LlbjNulmCK6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df1[\"Result\"].map(lambda x : [ c   for c in x  if c not in punc] )\n",
        "# df1[\"Result\"].map(lambda x : \"\".join([ c   for c in x  if c not in punc]))\n",
        "df1[\"Result\"] = df1[\"Result\"].map(lambda x : \"\".join([ c   for c in x  if c not in punc]))"
      ],
      "metadata": {
        "id": "MalVetTzCN5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()\n",
        "df1.drop(\"review\", axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "OuJGTVJDCRpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns\n",
        "df1.columns = ['sentiment', 'Review']"
      ],
      "metadata": {
        "id": "Ul4BloTbCkwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"Review\"]"
      ],
      "metadata": {
        "id": "QwIsoKo7Ct0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df1[\"Review\"][0]"
      ],
      "metadata": {
        "id": "2T-sYbZzCvvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word_tokenize(df1[\"Review\"][0])\n",
        "# df1[\"Review\"].map(lambda x : word_tokenize(x) )\n",
        "df1[\"Tokens\"] = df1[\"Review\"].map(lambda x : word_tokenize(x) )\n",
        "df1.head(3)"
      ],
      "metadata": {
        "id": "NygqhamAC2X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "sw_eng = stopwords.words(\"english\")\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "ZGV3OXoNC4qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens\n",
        "# df1[\"Tokens\"][0]\n",
        "# len(df1[\"Tokens\"][0])\n",
        "# [ w  for w in df1[\"Tokens\"][0] if w not in sw_eng]\n",
        "# len([ w  for w in df1[\"Tokens\"][0] if w not in sw_eng])\n",
        "# df1[\"Tokens\"].map( lambda x : [w    for w in x  if w not in sw_eng])\n",
        "\n",
        "df1[\"Tokens\"] = df1[\"Tokens\"].map( lambda x : [w    for w in x  if w not in sw_eng])"
      ],
      "metadata": {
        "id": "1Ny1ZxvCDGVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "port_stem = PorterStemmer()\n",
        "# df1[\"Tokens\"].map(lambda x :  [port_stem.stem(w)  for w in x ])\n",
        "\n",
        "df1[\"Tokens\"] = df1[\"Tokens\"].map(lambda x :  \" \".join([port_stem.stem(w)  for w in x ]))"
      ],
      "metadata": {
        "id": "M2ixToI3DX8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head(5)"
      ],
      "metadata": {
        "id": "QyOUUab-DvPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 73- Automate NLP Pipeline Process - (1st July)**"
      ],
      "metadata": {
        "id": "M3VYUPyGD_Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TRjElM6AD37h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-6gnMweEWj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHHUsCeVEWt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqTo8uHCEW0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 74- NGrams- (2nd July)**"
      ],
      "metadata": {
        "id": "aB-Oodo9EYJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "sent = \"Data Scientist work on Machine Learning and Deep Learning\"\n",
        "sent.split()"
      ],
      "metadata": {
        "id": "BKOj3V0bEW9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unigram\n",
        "ngrams( sent.split(), 1)"
      ],
      "metadata": {
        "id": "uxHcddLPEsGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(ngrams( sent.split(), 1))"
      ],
      "metadata": {
        "id": "pAky7b9qEvl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram\n",
        "ngrams(sent.split(),2)"
      ],
      "metadata": {
        "id": "brVMUZhjExs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(ngrams(sent.split(),2))"
      ],
      "metadata": {
        "id": "Hs4O-IV7E2nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tri - Gram\n",
        "list( ngrams( sent.split(), 3) )"
      ],
      "metadata": {
        "id": "cuS0n8EOE7Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 75- NER Recognition & POS Tagging- (3rd July)**"
      ],
      "metadata": {
        "id": "uSd_IxjTFDn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- POS (Part-of-Speech) tagging in NLP is the process of labeling each word in a sentence with its correct part of speech, such as noun, verb, adjective, etc., based on both its definition and context.\n",
        "- POS Tagging is a crucial step in NLP which helps model /\n",
        "system to understand grammatical structure and meaning\n",
        "of input sequence.\n",
        "- POS Tagging helps model with complete understanding\n",
        "of how words are related to each other.\n",
        "- POS Tagging helps the model with\n",
        "  - Understanding sequential structure of sentence.\n",
        "  - Remove ambiguity from the words (meaning)\n",
        "  - Extract Hidden context of the input sentence\n",
        "- POS Tagging is used\n",
        "  - Machine Translation\n",
        "  - Sentiment Analysis\n",
        "  - Information Retrieval\n",
        "  - Text Summarization"
      ],
      "metadata": {
        "id": "c6TOqi_NECBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('punkt_tab')\n",
        "# nltk.download('averaged_perceptron_tagger_eng')\n",
        "# nltk.download('tagsets_json')\n",
        "\n",
        "from nltk.help import brown_tagset\n",
        "from nltk.help import upenn_tagset\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "sent = \"The black cat jumps over green gate\"\n",
        "tokens = word_tokenize(sent)   # ['The', 'black', 'cat', 'jumps', 'over', 'green', 'gate']\n",
        "tokens_pos= nltk.pos_tag(tokens)\n",
        "tokens_pos"
      ],
      "metadata": {
        "id": "NZnAZfVrE_Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upenn_tagset(\"DT\")\n",
        "# upenn_tagset(\"JJ\")\n",
        "# upenn_tagset(\"NN\")"
      ],
      "metadata": {
        "id": "Zy8M50rpFJW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# for w , tag  in tokens_pos:\n",
        "#     print(w,\": ==>\", tag)\n",
        "#     res = upenn_tagset(tag)\n",
        "#     print(res)\n",
        "#     time.sleep(1)"
      ],
      "metadata": {
        "id": "Kh0XAaAKFPVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition**- NER is a very important technique in NLP which focus on identifying named entity within a sentence and segregate these entities into predefined categories.\n",
        "\n",
        "| Type    | Meaning                   | Example               |\n",
        "| ------- | ------------------------- | --------------------- |\n",
        "| PERSON  | Individual names          | Elon Musk             |\n",
        "| ORG     | Organizations/Companies   | Google, NASA          |\n",
        "| GPE     | Countries, cities, states | India, Paris, Texas   |\n",
        "| DATE    | Dates                     | July 11, 2025         |\n",
        "| TIME    | Time                      | 10:00 AM              |\n",
        "| MONEY   | Monetary values           | \\$100, ₹500           |\n",
        "| PERCENT | Percentages               | 20%                   |\n",
        "| PRODUCT | Products                  | iPhone, Tesla Model S |\n"
      ],
      "metadata": {
        "id": "b_miOZVmHx4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  nltk.download('maxent_ne_chunker_tab')\n",
        "# nltk.download('words')\n",
        "# !pip install -qU svgling\n",
        "\n",
        "from nltk import ne_chunk\n",
        "from nltk import pos_tag\n",
        "text = \"Apple Inc. is looking to buy a startup in America for $200 millions\"\n",
        "tokens = word_tokenize(text)\n",
        "tag_tokens = pos_tag(tokens)\n",
        "tag_tokens"
      ],
      "metadata": {
        "id": "zELRB7lxFf6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = ne_chunk(tag_tokens)\n",
        "ner"
      ],
      "metadata": {
        "id": "qvp3S7KLGHjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"Elon Musk is CEO of Tesla Inc. Located in America\"\n",
        "ne_chunk(pos_tag(word_tokenize(sent)))"
      ],
      "metadata": {
        "id": "-utYQeWiGcp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 76- BoW- (4th July)**"
      ],
      "metadata": {
        "id": "LQAU9wMLISHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bag of Words (BoW) is a simple and widely used text representation technique in NLP. It converts text into a numerical format so that machine learning algorithms can understand and work with it.\n",
        "\n",
        "- Vectorization in NLP refers to the process of converting textual data (words or documents) into numerical representations (vectors) so that machine learning models can process and learn from them.\n",
        "\n",
        "| Technique                  | Description                                                                                                                  | Example                                                                |\n",
        "| -------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |\n",
        "| **Bag of Words (BoW)**     | Counts the frequency of each word in the document.                                                                           | `[\"dog barks\", \"cat meows\"]` → `{dog:1, barks:1, cat:1, meows:1}`      |\n",
        "| **TF-IDF**                 | Adjusts word frequency by how rare a word is across all documents. Helps reduce the weight of common words like \"the\", \"is\". | Term Frequency × Inverse Document Frequency                            |\n",
        "| **One-Hot Encoding**       | Each word is represented by a binary vector with 1 at its index.                                                             | `[\"apple\", \"banana\", \"grape\"]` → apple = `[1,0,0]`, banana = `[0,1,0]` |\n",
        "| **Word Embeddings**        | Pre-trained vectors capturing word semantics. Words with similar meanings have similar vectors.                              | Word2Vec, GloVe, FastText                                              |\n",
        "| **Transformer Embeddings** | Context-aware vectors for words based on sentence structure.                                                                 | BERT, RoBERTa, GPT embeddings                                          |\n",
        "\n",
        "\n",
        "- Example: Suppose you have these 2 sentences:\n",
        "    - I love NLP\n",
        "    - I love machine learning\n",
        "    - Vocabulary (unique words): [I, love, NLP, machine, learning]\n",
        "\n",
        "Now, represent each sentence as a vector of word counts:\n",
        "\n",
        "| Sentence                  | I | love | NLP | machine | learning |\n",
        "| ------------------------- | - | ---- | --- | ------- | -------- |\n",
        "| `I love NLP`              | 1 | 1    | 1   | 0       | 0        |\n",
        "| `I love machine learning` | 1 | 1    | 0   | 1       | 1        |\n",
        "\n",
        "- compare BoW with Word2Vec, TF-IDF, or Transformer-based embeddings like BERT."
      ],
      "metadata": {
        "id": "_v6XbhSaM4WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cvec = CountVectorizer()  # lower case,\n",
        "cvec"
      ],
      "metadata": {
        "id": "avl4v7pvG4OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [ \"Tall brown Dog is playing in brown Green Fields\",\n",
        "         \"White Rabbit eating green Grass near rabbit cage\",\n",
        "          \"Brown Dog chase white rabbit\",\n",
        "          \"White Rabbit escaped into Green Gate.\"]\n",
        "cvec.fit(sent)"
      ],
      "metadata": {
        "id": "7pseIB-WMMFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvec.get_feature_names_out()"
      ],
      "metadata": {
        "id": "v3XWXyKpMVC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvec.vocabulary_"
      ],
      "metadata": {
        "id": "Vk9ZUnhjMcDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvec.transform(sent)"
      ],
      "metadata": {
        "id": "ZwLNYX0aMgQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ary=cvec.transform(sent).toarray()\n",
        "ary"
      ],
      "metadata": {
        "id": "h4bbRKmjMilE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(ary , columns = ['brown', 'cage', 'chase', 'dog', 'eating', 'escaped', 'fields',\n",
        "       'gate', 'grass', 'green', 'in', 'into', 'is', 'near', 'playing',\n",
        "       'rabbit', 'tall', 'white'] )"
      ],
      "metadata": {
        "id": "XQKCqRnmMmFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "docs = [\"I love NLP nlp machine\", \"I love machine learning\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(docs)\n",
        "pd.DataFrame(X.toarray() , columns = vectorizer.get_feature_names_out() )"
      ],
      "metadata": {
        "id": "pBqbWZvgMoDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "74evn1U9CF0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 77- TF-IDF- (7th July)**"
      ],
      "metadata": {
        "id": "GA6ASAIOOGpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is another popular technique used for vectorization in\n",
        "NLP.\n",
        "- It is a statistical measure which is used in NLP to extract\n",
        "information from the sequence data (information retrieval)\n",
        "to evaluate the importance of each word in a document\n",
        "with the collection of documents (corpus).\n",
        "\n",
        "- TF ==> Term Frequency\n",
        "  - This measures how frequently a term / word appears in a specific document.\n",
        "  - More number of times a particular document appears , that word is highly\n",
        "weighted as important word in that particular document\n",
        "- IDF ==> Inverse Document Frequency\n",
        "  - This measures how rarely or commonly a term / word appears across entire corpus\n",
        "  - Word that appears more number of times across all documents is considered as least important / prioritize\n",
        "  - Word that appears less number of times across all documents (corpus) is considered as highly prioritize .\n",
        "- TF = how important a word is within a document\n",
        "- IDF = how unique that word is across all documents (corpus)\n",
        "\n",
        "| Term                       | High TF | High IDF | Importance |\n",
        "| -------------------------- | ------- | -------- | ---------- |\n",
        "| Common word (e.g. \"the\")   | ✅       | ❌        | Low        |\n",
        "| Rare word (e.g. \"quantum\") | ✅       | ✅        | High       |\n"
      ],
      "metadata": {
        "id": "mnCmbVC2PIt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example corpus\n",
        "\n",
        "| Document | Content                   |\n",
        "| -------- | ------------------------- |\n",
        "| Doc1     | \"I love NLP\"              |\n",
        "| Doc2     | \"I love machine learning\" |\n",
        "\n",
        "- Step 1\n",
        "  - vocabulary- [I, love, NLP, machine, learning]\n",
        "- Step 2- Term Frequency (TF) = (Number of times a particular word appears in entire document) / (Total words in that document)\n",
        "\n",
        "| Word     | TF in Doc1  | TF in Doc2 |\n",
        "| -------- | ----------- | ---------- |\n",
        "| I        | 1/3 = 0.333 | 1/4 = 0.25 |\n",
        "| love     | 1/3 = 0.333 | 1/4 = 0.25 |\n",
        "| NLP      | 1/3 = 0.333 | 0          |\n",
        "| machine  | 0           | 1/4 = 0.25 |\n",
        "| learning | 0           | 1/4 = 0.25 |\n",
        "\n",
        "- Step 3: Inverse Document Frequency (IDF)= log(Total Docs in corpus  / (1 + Number of Docs containing word))\n",
        "(Total Docs = 2)\n",
        "\n",
        "| Word     | Docs Appeared In | IDF                           |\n",
        "| -------- | ---------------- | ----------------------------- |\n",
        "| I        | 2                | log(2 / (1+2)) = log(2/3)     |\n",
        "| love     | 2                | log(2 / (1+2)) = log(2/3)     |\n",
        "| NLP      | 1                | log(2 / (1+1)) = log(2/2) = 0 |\n",
        "| machine  | 1                | log(2 / (1+1)) = 0            |\n",
        "| learning | 1                | log(2 / (1+1)) = 0            |\n",
        "\n",
        " Often we use log base e or 10 and smooth the denominator with +1.\n",
        "\n",
        "- Step 4: TF × IDF- Multiply TF and IDF for each word per document:\n",
        "\n",
        "| Word     | TF-IDF Doc1        | TF-IDF Doc2     |\n",
        "| -------- | ------------------ | --------------- |\n",
        "| I        | 0.333 × log(2/3)   | 0.25 × log(2/3) |\n",
        "| love     | 0.333 × log(2/3)   | 0.25 × log(2/3) |\n",
        "| NLP      | 0.333 × log(2/2)=0 | 0               |\n",
        "| machine  | 0                  | 0.25 × 0 = 0    |\n",
        "| learning | 0                  | 0.25 × 0 = 0    |\n",
        "\n",
        "| Word     | Doc1 TF-IDF | Doc2 TF-IDF |\n",
        "| -------- | ----------- | ----------- |\n",
        "| I        | Low         | Low         |\n",
        "| love     | Low         | Low         |\n",
        "| NLP      | High        | 0           |\n",
        "| machine  | 0           | High        |\n",
        "| learning | 0           | High        |\n",
        "\n",
        "\n",
        "✅ Interpretation:\n",
        "- Common words (like \"I\", \"love\") → Low TF-IDF\n",
        "- Unique words (like \"NLP\", \"machine\", \"learning\") → Higher TF-IDF\n",
        "\n"
      ],
      "metadata": {
        "id": "bCEQV6G6PIqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfvec = TfidfVectorizer() #creating an instance of the vectorizer, but you still need to fit it on a list of documents to generate the TF-IDF values.\n",
        "tfvec"
      ],
      "metadata": {
        "id": "hugQVTBvN2QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = [ \"Tall brown Dog is playing in Green Fields\",\n",
        "         \"White Rabbit eating green Grass near rabbit cage\",\n",
        "          \"Brown Dog chase white rabbit\",\n",
        "          \"White Rabbit escaped into Green Gate.\"]\n",
        "tfvec.fit(sent)"
      ],
      "metadata": {
        "id": "HyLX624rOqF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfvec.get_feature_names_out() #returns the list of unique terms (vocabulary) extracted from your corpus"
      ],
      "metadata": {
        "id": "NnUnhluzOsN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfvec.vocabulary_  # Python dictionary mapping each term (word) in the vocabulary to its column index in the TF-IDF matrix."
      ],
      "metadata": {
        "id": "dy2gh49POwq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfvec.transform(sent)"
      ],
      "metadata": {
        "id": "VN67VaiDOyhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = tfvec.transform(sent).toarray()\n",
        "out"
      ],
      "metadata": {
        "id": "C7VaNG41O0aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(out , columns=['brown', 'cage', 'chase', 'dog', 'eating', 'escaped', 'fields',\n",
        "       'gate', 'grass', 'green', 'in', 'into', 'is', 'near', 'playing',\n",
        "       'rabbit', 'tall', 'white'] )"
      ],
      "metadata": {
        "id": "vTCbvbFjO16Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 78- Embeddings pdf- (8th July)**"
      ],
      "metadata": {
        "id": "wFD4TJVDwHk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 79- Word2Vec- (9th July)**"
      ],
      "metadata": {
        "id": "faCnGFhdwbHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gensim\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Pre-Trained Word2Vec Embedding Model, File-size 1.5 GB,Vector Size 300 Dimensions,Corpus ==> 100 Billion words\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "model"
      ],
      "metadata": {
        "id": "E-eM8yGvBQq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "king_vector= model[\"king\"]\n",
        "len(king_vector)"
      ],
      "metadata": {
        "id": "WJ-FYMpKws84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "laptop_vector = model[\"laptop\"]\n",
        "\n",
        "model.most_similar(\"computer\")\n",
        "model.similarity(\"man\",\"king\")\n",
        "model.similarity(\"queen\",\"king\")\n",
        "model.similarity(\"doctor\", \"hospital\")\n",
        "model.doesnt_match([\"java\", \"php\", \"dog\"])\n",
        "result = model[\"king\"] - model[\"man\"] + model[\"woman\"]\n",
        "model.most_similar(result)"
      ],
      "metadata": {
        "id": "crj1JVIRxQVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 80- GloVe Embeddings- (10th July)**"
      ],
      "metadata": {
        "id": "5zbcBitoyJua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gensim\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
        "glove_model"
      ],
      "metadata": {
        "id": "uahUtGUiyPhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model[\"lion\"]\n",
        "len(glove_model[\"lion\"])"
      ],
      "metadata": {
        "id": "62ssrxuOy5TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model[\"love\"]\n",
        "glove_model.most_similar(\"car\")\n",
        "glove_model.most_similar(\"car\", topn = 5 )\n",
        "glove_model[\"husband\"]\n",
        "glove_model[\"wife\"]\n",
        "glove_model[\"man\"]\n",
        "glove_model[\"woman\"]\n",
        "# husband - man + woman\n",
        "\n",
        "# positive ==>  husband + woman\n",
        "# negative  ==> man\n",
        "\n",
        "res = glove_model.most_similar( positive = [\"husband\", \"woman\"], negative = [\"man\"], topn = 2)\n",
        "res\n",
        "\n",
        "res1 = glove_model.most_similar( positive = [\"king\", \"woman\"], negative = [\"man\"], topn = 5)\n",
        "res1"
      ],
      "metadata": {
        "id": "O6AA0y5JzRKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **FastText**"
      ],
      "metadata": {
        "id": "V0ZyYR-y_L0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "      - FastText is a powerful and efficient library developed by Facebook AI Research (FAIR) for text classification and word representation in NLP.\n",
        "      - It is word embedding technique which is specifically designed to handle rare words.\n",
        "      - In this words are further divided into subwords to understand relationship between subwords.\n",
        "      - It is kind of enhanced version of word2vec which can handle rare words very efficiently or words which are which are never seen before.  like playng.\n",
        "      - working mechanism-\n",
        "        - Tokenization\n",
        "        - each word after tokenization further divided into subwords based on window of characters\n",
        "        - Fasttext combines all its subwords vectors create the final word vector\n",
        "        - It performs the prediction of target word"
      ],
      "metadata": {
        "id": "vQH1UicH6FX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"The cat sat on the mat.\n",
        "Dogs are loyal animals.\n",
        "I love learning about natural language processing.\n",
        "FastText is great for text classification.\n",
        "Birds can fly in the sky.\n",
        "Apples and oranges are fruits.\n",
        "The sun rises in the east.\n",
        "Python is a popular programming language.\n",
        "She is reading a book under the tree.\n",
        "AI is changing the world rapidly.\"\"\"\n",
        "\n",
        "with open(\"data.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "\n",
        "# !pip install -qU fasttext\n",
        "import fasttext  # importing FastText library, which allows you to train and use word embeddings or text classifiers.\n",
        "\n",
        "# Train Skip-gram model\n",
        "model = fasttext.train_unsupervised('data.txt', model='cbow')  #model= cbow/ skipgram\n",
        "\n",
        "# Get word vector\n",
        "print(model.get_word_vector(\"language\")) #This retrieves the embedding vector for the word \"language\". The result is a 100-dimensional float vector by default.\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZjiHJIt-8Jjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Step                      | Purpose                            |\n",
        "| ------------------------- | ---------------------------------- |\n",
        "| `train_unsupervised`      | Train word vectors from raw text   |\n",
        "| `'skipgram'`              | Predict context from a center word |\n",
        "| `get_word_vector(\"word\")` | Fetch the learned vector of a word |\n",
        "\n",
        "| Feature                      | FastText         | Word2Vec / GloVe   |\n",
        "| ---------------------------- | ---------------- | ------------------ |\n",
        "| Handles OOV words            | ✅ Yes            | ❌ No               |\n",
        "| Uses subword information     | ✅ Yes            | ❌ No               |\n",
        "| Fast training                | ✅ Yes            | ✅ Yes              |\n",
        "| Morphological awareness      | ✅ Strong         | ❌ Weak             |\n",
        "| Pre-trained models available | ✅ 157+ languages | ✅ English (mainly) |\n"
      ],
      "metadata": {
        "id": "qHxd66UP9Myw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 81- Sequence Data pdf- (12th July)**"
      ],
      "metadata": {
        "id": "I98uIOWxzp9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    - CNN (Convolutional Neural Network) ==> Images\n",
        "    - RNN (Recurrent Neural Network) ==> Sequence Data\n",
        "\n",
        "      **Sequence Data** : type of data where order of the words in the sequence is very important. Each word in the sequence are related to surrounding elements (before word and after word.). This type of data is often collected over time.\n",
        "\n",
        "- Why Sequence Matters\n",
        "\n",
        "| Sentence A                   | Sentence B                    |\n",
        "| ---------------------------- | ----------------------------- |\n",
        "| \"He didn’t say he stole it.\" | \"He said he didn’t steal it.\" |\n",
        "      Same words, different order, different meaning. This is why models need to capture the sequence of words — not just their presence.\n",
        "\n",
        "- Example of Sequence data\n",
        "      - Textual data which is arranged in proper order. ex- Sentences, - Paragraphs, Documents, Articles, Chat Conversation....etc.\n",
        "      - Time Series Data- Data Collected based on time i.e regular  intervals of time. ex- Sales prediction Data, Temperature prediction, stock Price Prediction...etc.\n",
        "\n",
        "| Type of Text      | Sequence Example                               |\n",
        "| ----------------- | ---------------------------------------------- |\n",
        "| Sentence          | \"The cat sat on the mat.\"                      |\n",
        "| Chat conversation | \"Hi\" → \"Hello!\" → \"How are you?\"               |\n",
        "| Paragraph         | Context builds over multiple sentences         |\n",
        "| Document          | News articles, essays, or instructions         |\n",
        "| Code or commands  | \"if\", \"else\", \"print\", etc. (sequence matters) |\n",
        "\n",
        "- Summary\n",
        "      - Sequential data is core to NLP — word order shapes meaning.\n",
        "      - NLP models must preserve or learn this order to perform well.\n",
        "      - Sequence-based models (RNNs, Transformers) are built to handle this structured dependency.\n",
        "\n",
        "- Key Features of Sequential Data\n",
        "      - Order Matters : Position of an element within the sequence is important to understand overall meaning and context. Example: \"I only eat fruit\" ≠ \"Only I eat fruit\"\n",
        "      - Sequential Relationship : (Temporal Relationship) It explains the relationships of a particular element with its before and after elements. Example: In \"She was not happy\", the word \"not\" modifies \"happy\".\n",
        "      - Variable Length : In sequence data each sequence has different length which effects the contextual meaning of sequence and also in understanding relationship among the words. e.g., short tweets vs long articles.Longer sequences often require memory or attention mechanisms to understand distant relationships.\n",
        "      - Dependencies : Each element in the sequence often dependent on surrounding elements.Example: In \"If it rains, we will cancel the picnic\", the second part depends on the first.\n",
        "      - Contextual Sensitivity : The meaning of each element depends on the context created by the rest of the sequence. This is why simple bag-of-words models often fail—because they lose sequence and context.\n",
        "\n",
        "- Models which deals with Sequence data is called Seq2Seq\n",
        "  Model.\n",
        "      A Sequence-to-Sequence (Seq2Seq) model is a neural architecture designed to transform one sequence into another. It’s widely used when both the input and output are sequences, possibly of different lengths.\n",
        "\n",
        "      Basic Model which deals with Sequence data-\n",
        "        RNN ==> Recurrent Neural Network\n",
        "        LSTM ==> Long Short Term Memory.\n",
        "\n",
        "| Task                        | Input Sequence               | Output Sequence                     |\n",
        "| --------------------------- | ---------------------------- | ----------------------------------- |\n",
        "| Machine Translation         | `\"I love NLP\"`               | `\"J'aime le traitement du langage\"` |\n",
        "| Text Summarization          | `\"The article discusses...\"` | `\"Summary of the article\"`          |\n",
        "| Chatbot Response Generation | `\"Hi, how are you?\"`         | `\"I'm good, thanks!\"`               |\n",
        "| Speech Recognition          | `Audio waveform`             | `\"Hello world\"`                     |\n",
        "| Code Generation             | `\"Write a Python function\"`  | `\"def add(a, b): return a+b\"`       |\n",
        "\n",
        "\n",
        "\n",
        "| Model Type                         | How It Works                                         |\n",
        "| ---------------------------------- | ---------------------------------------------------- |\n",
        "| **RNN (Recurrent Neural Network)** | Reads tokens one by one and maintains state          |\n",
        "| **LSTM / GRU**                     | Improved RNNs for long-range dependencies            |\n",
        "| **CNN (1D)**                       | Detects local n-gram features, ignores global order  |\n",
        "| **Transformer**                    | Uses attention to relate all positions at once       |\n",
        "| **BERT/GPT**                       | Pretrained Transformers that understand full context |\n"
      ],
      "metadata": {
        "id": "rQySdYxmFsFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 82- RNN pdf- (14th July)**"
      ],
      "metadata": {
        "id": "Ud_3rpADz7BI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "       Hierarchy of how machine learning and deep learning models evolve to address increasingly complex data problems\n",
        "       ML → ANN → CNN → RNN\n",
        "       \n",
        "        Machine Learning Models\n",
        "        - Can deal only small volume of Dataset\n",
        "        - Needed: Feature Extraction, Feature Selection, Error Rectification, Encoding, Imbalance....etc\n",
        "        - Disadvantage :\n",
        "          - We can't handle large volume of data.\n",
        "          - There is not Auto Feature Extraction\n",
        "          - It can't handle large number of features\n",
        "        - Solution : Feed Forward Neural Network (ANN)\n",
        "                        - Auto Feature Engineering\n",
        "                        - Auto feature extraction\n",
        "                        - It can handle large volume of data\n",
        "\n",
        "        Artificial Neural Network :\n",
        "          - Feed Forward Neural Network (ANN)\n",
        "          - Auto Feature Engineering\n",
        "          - Auto feature extraction\n",
        "          - It can handle large volume of data\n",
        "          - It has feature like Weights, Activation Function, Bias, Forward Propagation and Back Propagation.\n",
        "          - Problem with ANN- can't deal with\n",
        "            - Complex Multi Dimension Data, Image Recognition, Speech Conversation, Dataset with Huge Input Parameter\n",
        "          - Solution :\n",
        "            - CNN can rectify all problem of ANN.\n",
        "            - Image\n",
        "            - Huge Input Parameter\n",
        "            - Complex multi dimensional data.\n",
        "          CNN\n",
        "            - stands for Convolutional Neural Network which handle complex multi dimensional data like colour images.\n",
        "            - Image Classification, Image Segmentation, Image Recognition\n",
        "            - Huge Input Parameter\n",
        "            - Complex multi dimensional data.\n",
        "            - Limitation of CNN : CNN can't deal with\n",
        "              - Sequential data, Time Series dat, Text data,Language translation, Text Summarization, Sentiment Analysis....etc.\n",
        "              - No memory provided: CNNs do not have memory of past inputs. They are stateless and process each input independently.\n",
        "              - Memory in neural networks refers to the ability to retain information about previous inputs in a sequence. This is crucial for sequential data like text, time series, and speech — where context builds over time.\n",
        "\n",
        "            - Solution :\n",
        "              - RNN can handle Sequential data problems\n",
        "              - Time Series, Language Translation\n",
        "              - Text Summarization\n",
        "              - Sentiment Analysis...etc.\n",
        "\n",
        "              For text:\n",
        "              CNN might see:\n",
        "              \"He is very not good\"\n",
        "              - CNN might treat “not” and “good” separately or in a small window — but not remember their relation across the whole sentence.\n",
        "              - RNN would process one word at a time and maintain memory:\n",
        "                  \"He → is → very → not → good\"\n",
        "                  So it understands the negation. Understanding t, t+1, t+2 in Sequential Data\n",
        "\n",
        "| Model Type                             | Best For                            | Key Features                                                                                        | Limitations                                                                                                     | Solution                        |\n",
        "| -------------------------------------- | ----------------------------------- | --------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ------------------------------- |\n",
        "| **ML (Machine Learning)**              | Small structured/tabular data       | - Manual feature engineering<br>- Simple algorithms<br>- Fast on small data                         | - Can't handle large data<br>- No auto feature extraction<br>- Struggles with unstructured data                 | ➡️ Use **ANN**                  |\n",
        "| **ANN (Artificial Neural Network)**    | Large structured data               | - Auto feature extraction<br>- Forward & backpropagation<br>- Handles high dimensionality           | - Not effective for images<br>- Struggles with spatial data<br>- Weak with high-dimensional input (like images) | ➡️ Use **CNN**                  |\n",
        "| **CNN (Convolutional Neural Network)** | Images, spatial data                | - Detects patterns via filters<br>- Efficient with large input<br>- Handles 2D/3D data              | - Not suitable for sequences<br>- Can't handle time series, text, speech                                        | ➡️ Use **RNN**                  |\n",
        "| **RNN (Recurrent Neural Network)**     | Sequential data: text, time, speech | - Maintains memory across steps<br>- Learns temporal patterns<br>- Works with variable-length input | - Training can be slow<br>- Can forget long dependencies (vanishing gradient)                                   | ➡️ Use **LSTM/GRU/Transformer** |"
      ],
      "metadata": {
        "id": "9RK7Te1NDy5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "      ANN Architecture Summary Table\n",
        "\n",
        "| Layer Type          | Purpose                          | Activation Function      | # of Neurons             |\n",
        "| ------------------- | -------------------------------- | ------------------------ | ------------------------ |\n",
        "| **Input Layer**     | Accept raw data                  | None                     | Equal to # of features   |\n",
        "| **Hidden Layer(s)** | Learn patterns & transformations | ReLU, Sigmoid, Tanh      | Tunable (hyperparameter) |\n",
        "| **Output Layer**    | Give predictions                 | Sigmoid / Softmax / None | Depends on target type   |\n",
        "\n",
        "\n",
        "| Feature                    | **ANN (Artificial Neural Network)**                      | **CNN (Convolutional Neural Network)**                            | **RNN (Recurrent Neural Network)**                         |\n",
        "| -------------------------- | -------------------------------------------------------- | ----------------------------------------------------------------- | ---------------------------------------------------------- |\n",
        "| **Purpose**                | General-purpose, structured data                         | Spatial data like images                                          | Sequential data like text, time series, speech             |\n",
        "| **Input Type**             | Fixed-size vector input                                  | Grid-like (e.g., image pixels)                                    | Sequences (e.g., words, time steps)                        |\n",
        "| **Layer Types**            | Dense (Fully Connected)                                  | Convolutional + Pooling + Dense                                   | Recurrent (looped) + Dense                                 |\n",
        "| **Memory / Context**       | ❌ No memory of past inputs                               | ❌ No memory (local features only)                                 | ✅ Yes – remembers past inputs via hidden states            |\n",
        "| **Handles Sequence**       | ❌ No                                                     | ❌ No                                                              | ✅ Yes                                                      |\n",
        "| **Handles Image Data**     | ✅ But not optimized                                      | ✅ Specialized for images                                          | ❌ Not suitable                                             |\n",
        "| **Handles Text/Time Data** | ❌ Not well                                               | ✅ Limited (with filters)                                          | ✅ Best suited                                              |\n",
        "| **Computation**            | Low to Medium                                            | Medium to High                                                    | High (due to time-step loops)                              |\n",
        "| **Parallelism (Training)** | ✅ High                                                   | ✅ High                                                            | ❌ Low – sequential processing                              |\n",
        "| **Use Cases**              | - Tabular data<br>- Regression<br>- Basic classification | - Image classification<br>- Object detection<br>- Medical imaging | - Text generation<br>- Translation<br>- Sentiment analysis |\n",
        "| **Weight Sharing**         | ❌ No                                                     | ✅ Yes (shared kernels)                                            | ✅ Shared across time steps                                 |\n",
        "| **Example Algorithms**     | Logistic Regression, MLP                                 | LeNet, AlexNet, VGG, ResNet                                       | Vanilla RNN, LSTM, GRU                                     |\n",
        "| **Strength**               | Simplicity                                               | Local spatial feature extraction                                  | Temporal/contextual modeling                               |\n",
        "| **Weakness**               | No spatial or temporal awareness                         | No memory for sequences                                           | Vanishing gradient (in vanilla RNN)                        |\n",
        "\n"
      ],
      "metadata": {
        "id": "PhcqgkUsKVJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### RNN\n",
        "\n",
        "      RNN :\n",
        "      - A Recurrent Neural Network (RNN) is type of Artificial Neural Network designed to handle sequential data.\n",
        "      - As RNNs have internal memory, allowing them to process sequences of inputs by retaining information about previous input elements in the sequence.\n",
        "      - This \"internal memory\" makes them suitable for tasks where order of input data matters or important. Such as natural language processing and speech recognition.\n",
        "      - Unfolded vs Folded RNN\n",
        "\n",
        "    🧩 1. Input Sequence: Suppose input is a sequence of words (or time steps):\n",
        "    X = [x₁, x₂, x₃, ..., xₜ] . Each input xₜ is passed into the RNN one at a time (not all at once like ANN).\n",
        "\n",
        "      2. Hidden State Update: At every time step t, the RNN:\n",
        "\n",
        "          - Receives current input xₜ\n",
        "          - Receives previous hidden state hₜ₋₁\n",
        "          - Produces new hidden state hₜ\n",
        "          - hₜ = f(Wₓ * xₜ + Wₕ * hₜ₋₁ + b)\n",
        "          -W-weight, b-bias, f- activation function (tanh or ReLU),hₜ- updated memory\n",
        "          - This allows the model to remember what it saw before"
      ],
      "metadata": {
        "id": "jBR35vuEyo9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 83- RNN- (21st July)**"
      ],
      "metadata": {
        "id": "qkLEUOXw0-Bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "      Evolution of RNN :\n",
        "      In 1980 ==> John Hopfield ==> Hopfield Network\n",
        "              - In Hopfield network multiple neural networks are connected with each other over the feedback. Feedback of previous neural network is pass on the next neural network as input with which pattern is\n",
        "              remembered over the time in sequential data.\n",
        "              - Draws of Hopfield Network was rectified by David Rumelhart with the help of Geoffrey Hinton by introducing BPTT (Back Propagation Through Time)\n",
        "      - LSTM : 1997 - 2010: Long Short Term Memory. Complicated architecture consists of\n",
        "              - Forget Gate\n",
        "              - Input Gate\n",
        "              - Output Gate\n",
        "              - Cell State\n",
        "              - Hidden State\n",
        "      - GRU (Gated Recurrent Unit)  ==> Equally Powerful as LSTM with Simple Architecture ==> 2014\n",
        "      - Encoder - Decoder Architecture- Ilya Sutskever. LSTM ==> multiple\n",
        "      - Attention Mechanism\n",
        "      - Self Attention Mechanism\n",
        "      - Transformer Model-> BERT & GPT\n",
        "\n",
        "| **Year**    | **Contributor(s)**           | **Model / Milestone**      | **Contribution**                                                                      | **Summary**                                               | **Drawbacks**                                                  |\n",
        "| ----------- | ---------------------------- | -------------------------- | ------------------------------------------------------------------------------------- | --------------------------------------------------------- | -------------------------------------------------------------- |\n",
        "| 1980        | **John Hopfield**            | **Hopfield Network**       | Introduced feedback-based network. Remembered patterns using energy minimization.     | Early recurrent model with memory for fixed patterns      | Limited memory, not suitable for sequence prediction           |\n",
        "| 1986        | **Rumelhart & Hinton**       | **Backpropagation + BPTT** | Extended backpropagation for RNNs using time-unrolling                                | Enabled training RNNs using BPTT                          | Prone to vanishing/exploding gradients                         |\n",
        "| Early 1990s | **Sepp Hochreiter**          | **RNN Gradient Analysis**  | Found major training issues with deep RNNs (vanishing gradient)                       | Highlighted need for better architectures                 | Couldn’t solve long-term dependency problems                   |\n",
        "| 1997        | **Hochreiter & Schmidhuber** | **LSTM**                   | Introduced memory cells, input/output/forget gates to preserve long-term dependencies | First practical solution for long sequence learning       | Computationally intensive, complex architecture                |\n",
        "| 2014        | **Kyunghyun Cho et al.**     | **GRU**                    | Simplified LSTM with reset and update gates, faster training                          | Lightweight alternative to LSTM                           | Slightly less accurate on some tasks than LSTM                 |\n",
        "| 2014–2017   | **Google, Facebook, etc.**   | **Attention Mechanism**    | Allowed models to focus on relevant input tokens during decoding                      | Improved translation and summarization accuracy           | Still depends on RNN backbone (before Transformers)            |\n",
        "| 2017        | **Google**                   | **Transformer**            | Fully attention-based model with parallel processing, no recurrence                   | Most powerful and scalable architecture (e.g., BERT, GPT) | Requires large data and compute, lacks inherent temporal order |\n"
      ],
      "metadata": {
        "id": "kioHWAQr21fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    🔁 RNN Variants Based on Input/Output Architecture\n",
        "\n",
        "| **Type**                        | **Input**      | **Output**                         | **Use Case**                          |\n",
        "| ------------------------------- | -------------- | ---------------------------------- | ------------------------------------- |\n",
        "| **One-to-One**                  | Single         | Single                             | Image classification                  |\n",
        "| **One-to-Many**                 | Single         | Sequence                           | Image captioning                      |\n",
        "| **Many-to-One**                 | Sequence       | Single                             | Sentiment analysis                    |\n",
        "| **Many-to-Many (synchronous)**  | Sequence       | Sequence                           | POS tagging, named entity recognition |\n",
        "| **Many-to-Many (asynchronous)** | Input sequence | Output sequence (different length) | Machine translation, summarization    |\n"
      ],
      "metadata": {
        "id": "12ZT85p08SOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 84- RNN Implementation- (22 July)**\n",
        "\n",
        "      RNN Implementation on Twitter Sentiment Data (from the given URL)"
      ],
      "metadata": {
        "id": "9IzzpgNqGskL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    TensorFlow is an open-source machine learning and deep learning framework developed by Google. It allows developers to build, train, and deploy machine learning models easily — especially neural networks.\n",
        "    ✅ Build and train models for:\n",
        "      Image classification (e.g., recognizing cats vs dogs)\n",
        "      Natural Language Processing (e.g., chatbots, translation)\n",
        "      Time series forecasting\n",
        "      Speech recognition\n",
        "      Object detection\n",
        "      Recommender systems\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RPCdSIpBLrET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "# import contractions\n",
        "import nltk\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head(5)\n",
        "\n",
        "# label - dependent coolumn (0 = negative, 1 = positive)\n",
        "# tweet- independent column"
      ],
      "metadata": {
        "id": "5q2Fo5QOz6Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.drop(\"id\", axis = 1, inplace = True)  # don't require ID column\n",
        "# df[\"tweet\"]\n",
        "# df[\"tweet\"][0]\n",
        "# df[\"tweet\"][3]\n",
        "df.shape"
      ],
      "metadata": {
        "id": "cwoMa7t8HU4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()# summary\n",
        "df.label.value_counts()    #oversampling- undersampling required to balance the data"
      ],
      "metadata": {
        "id": "bqjoiRvSH7Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[[\"tweet\"]]\n",
        "y= df[\"label\"]"
      ],
      "metadata": {
        "id": "QqlmqJt7S8rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler  # RandomOverSampler is used to handle imbalanced datasets by randomly duplicating examples from the minority class until both classes are balanced.\n",
        "ros = RandomOverSampler()\n",
        "ros"
      ],
      "metadata": {
        "id": "OCvHHzYyTDMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ros, y_ros = ros.fit_resample(X, y)\n",
        "y_ros.value_counts()"
      ],
      "metadata": {
        "id": "EC2oOr2eTK1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame(X_ros)\n",
        "df1['label'] = y_ros\n",
        "df1.head(3)"
      ],
      "metadata": {
        "id": "1bw9yaWvTaMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords\n",
        "swords = stopwords.words(\"english\")  # downloading all stopwords\n",
        "len(swords)\n",
        "swords"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mhyhG8HBTyWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"tweet\"] = df1[\"tweet\"].map( lambda x : x.lower())\n",
        "df1[\"tweet\"] = df1[\"tweet\"].map(lambda x :  re.sub(r\"@\\w+|#\\w+\",\"\", x))   # applying a regex substitution to remove usernames (@user) and hashtags (#tag) from tweets\n",
        "df1[\"tweet\"] = df1[\"tweet\"].map(lambda x :  re.sub(\"http\\S+|www\\S+|https\\S+\",\"\",x)) # removes URLs from tweets\n",
        "df1[\"tweet\"]= df1[\"tweet\"].map(lambda x :   re.sub(\"[^a-zA-Z0-9 ]\", \"\", x) ) # Removes punctuation, special characters, emojis, etc\n",
        "df1[\"tweet\"] = df1[\"tweet\"].map(lambda x : \" \".join( [ w    for w in x.split()   if w not in swords] )) #remove stopwords\n",
        "\n",
        "# !pip install contractions\n",
        "import contractions\n",
        "df1[\"tweet\"]= df1[\"tweet\"].map(lambda x: contractions.fix(x))\n",
        "\n",
        "# check for automated preprocessing steps\n",
        "df1['tweet'].head(3)"
      ],
      "metadata": {
        "id": "p7xx6OKdTyYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WjljXSorn46o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 85- RNN Implementation Cont- (23rd July)**\n"
      ],
      "metadata": {
        "id": "tDodeoYbcbyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "      A tokenizer in NLP is a tool or function that breaks down text into smaller units called tokens — typically words, subwords, or characters.\n",
        "\n",
        "      # How Tokenizer Assigns Word Indexes- After calling tokenizer.fit_on_texts(texts), the tokenizer:\n",
        "        # this is different-> from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "        # Tokenizes (splits) all input texts into words (tokens).\n",
        "        # Counts frequency of each word.\n",
        "        # Sorts words by descending frequency (most frequent first).\n",
        "        # Assigns index starting from 1 (or 2 if oov_token is specified)."
      ],
      "metadata": {
        "id": "ML0IEKWQwoU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization using TensorFlow's Tokenizer. The Tokenizer transforms raw text (sentences or documents) into a format that machine learning models can understand — usually sequences of numbers.\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token= \"<00V>\" )  # Only the top 10,000 most frequent words will be kept.Any word not seen during training will be replaced with this special Out-Of-Vocabulary token.\n",
        "tokenizer.fit_on_texts(df1[\"tweet\"])  #This tells the tokenizer to analyze the given list of texts and Count word frequency, Build a vocabulary, Assign a unique integer index to each word based on its frequency.\n",
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HDye1aLqcf_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample tokenizer\n",
        "tokenizer1 = Tokenizer(num_words=10000, oov_token= \"<00V>\" )\n",
        "tokenizer1.fit_on_texts([\"I love NLP\", \"NLP is amazing\"])\n",
        "\n",
        "print(tokenizer1.word_index)\n",
        "seq=tokenizer1.texts_to_sequences([\"I love AI\", \"AI is Love NLP amazing\"])  # see OOV\n",
        "seq"
      ],
      "metadata": {
        "id": "YMHfPQhpHYMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It ensures that all sequences have the same length. This is necessary because neural networks require input tensors to be of uniform shape.\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequences = pad_sequences(seq, maxlen=10, padding = \"post\", truncating=\"post\")\n",
        "padded_sequences"
      ],
      "metadata": {
        "id": "mxXfXqmVIgWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(padded_sequences[0]), len(padded_sequences[1])"
      ],
      "metadata": {
        "id": "od2oAeyGQ3U5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"tweet\"].head(3)"
      ],
      "metadata": {
        "id": "d-QvmN0Fcf4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.texts_to_sequences(df1[\"tweet\"])\n",
        "sequences = tokenizer.texts_to_sequences(df1[\"tweet\"])\n",
        "sequences"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UI__HrbDcftH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing Padding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequences = pad_sequences(sequences, maxlen=50, padding = \"post\", truncating=\"post\")\n",
        "padded_sequences"
      ],
      "metadata": {
        "id": "11PF_6_jcfq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences.shape"
      ],
      "metadata": {
        "id": "vK_Z-XiYcfoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 86- RNN Implementation Cont- (24 July)**\n"
      ],
      "metadata": {
        "id": "WBYNzr2qSZzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data\n",
        "X = padded_sequences #type(X): numpy.ndarray\n",
        "y = df1[\"label\"].values  # print(type(y)): numpy.ndarray\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "X_train.shape\n",
        "X_test.shape\n",
        "y_train.shape\n",
        "y_test.shape\n",
        "type(X), type(y), type(df1[\"label\"])"
      ],
      "metadata": {
        "id": "QrVbPzFWcfl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Building\n",
        "\n",
        "# Importing the Sequential model and required layers: Embedding, SimpleRNN, and Dense.\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "smodel = Sequential()  # Initializes a sequential neural network (layer-by-layer model).\n",
        "\n",
        "smodel.add( Embedding(input_dim = 10000, output_dim = 64, input_length = 50))\n",
        "smodel.add(SimpleRNN(64))\n",
        "smodel.add(Dense(1, activation = \"sigmoid\"))\n",
        "\n",
        "smodel.summary()"
      ],
      "metadata": {
        "id": "C37R8z8DcfjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "      Embedding Layer: Converts word indices (integers) into dense vectors.\n",
        "        - input_dim=10000: Vocabulary size (max word index is 9999)\n",
        "        - output_dim=64: Each word will be represented as a 64-dimensional vector\n",
        "        - input_length=50: Input sequence length is 50 tokens\n",
        "        📌 Output Shape: (None, 50, 64)\n",
        "        \n",
        "      Simple RNN Layer: Processes the embedded sequence data one step at a time, maintaining hidden state.\n",
        "\n",
        "        - 64 is the number of RNN units (neurons)\n",
        "        - It returns the last hidden state, not the full sequence        📌 Output Shape: (None, 64)\n",
        "\n",
        "        Dense Layer: Output layer with 1 neuron\n",
        "          - activation=\"sigmoid\": Useful for binary classification (e.g., sentiment analysis)\n",
        "          📌 Output Shape: (None, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "pJ2yNdqloNmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smodel.compile(optimizer=\"adam\",\n",
        "               loss = \"binary_crossentropy\",\n",
        "               metrics = [\"accuracy\"] )\n",
        "smodel.fit(X_train, y_train, epochs = 10, batch_size = 64)"
      ],
      "metadata": {
        "id": "HG948xx2cfZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smodel.evaluate(X_test, y_test)  # accuracy score, 96% predict 96% times accuractely wehter tweet is +ve or -ve"
      ],
      "metadata": {
        "id": "pmK4OJo-qjYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y)"
      ],
      "metadata": {
        "id": "OjfksDiHSL_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary of RNN Programm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# 1. Load data\n",
        "url = \"https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# 2. Preprocess text and labels\n",
        "texts = df['tweet'].astype(str).tolist()\n",
        "labels = df['label'].values  # 0 = negative, 1 = positive\n",
        "\n",
        "# 3. Tokenization\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "# 4. Model definition\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=16, input_length=100),\n",
        "    SimpleRNN(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 5. Compile & train\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(padded_sequences, labels, epochs=5, batch_size=64, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "RJeHB95iIRUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGlK4jC7QT2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
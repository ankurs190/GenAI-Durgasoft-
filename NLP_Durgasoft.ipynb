{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K30XKrq5Ig3G",
        "rhUbyCjtmPuh",
        "Z95ZEX-8vTZn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### NLP Summary"
      ],
      "metadata": {
        "id": "pZ-uFyW75y5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Step                    | Description                                       | Tool/Library                | Example Input                                   | Example Output             |\n",
        "| ----------------------- | ------------------------------------------------- | --------------------------- | ----------------------------------------------- | -------------------------- |\n",
        "| **Lowercasing**         | Converts all text to lowercase                    | Python string methods       | \"Hello World\"                                   | \"hello world\"              |\n",
        "| **Tokenization**        | Splits text into words/sentences                  | `nltk.word_tokenize()`      | \"I love NLP.\"                                   | \\['I', 'love', 'NLP', '.'] |\n",
        "| **Stopword Removal**    | Removes common words with little meaning          | `nltk.corpus.stopwords`     | \"I am learning NLP\"                             | \\['learning', 'NLP']       |\n",
        "| **Punctuation Removal** | Removes symbols like `.,!?`                       | `string.punctuation` + `re` | \"Hello!\"                                        | \"Hello\"                    |\n",
        "| **Stemming**            | Trims words to their root form                    | `PorterStemmer`, `Snowball` | \"running\", \"flies\"                              | \"run\", \"fli\"               |\n",
        "| **Lemmatization**       | Converts word to dictionary base form using POS   | `WordNetLemmatizer`         | \"better\", \"was\"                                 | \"good\", \"be\"               |\n",
        "| **POS Tagging**         | Tags words with part of speech (noun, verb, etc.) | `nltk.pos_tag()`            | \"run\", \"beautiful\"                              | \\[('run', 'VB'), ...]      |\n",
        "| **Regex Cleaning**      | Custom text cleanup using patterns                | `re` module                 | \"[User123@gmail.com](mailto:User123@gmail.com)\" | \"User\" (after regex)       |\n"
      ],
      "metadata": {
        "id": "QzNtCHdn5t_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 69 (25-June)**"
      ],
      "metadata": {
        "id": "GcRklRwsz3bP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **String Punctuation**\n"
      ],
      "metadata": {
        "id": "K30XKrq5Ig3G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DJ7pADKSsOBl"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oFQupiCyscoK",
        "outputId": "e8bfa6a6-4431-4116-fe9d-bf2cf8e2aee0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punc = string.punctuation"
      ],
      "metadata": {
        "id": "9coT-5IfslkK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NI8nyBwDsu84",
        "outputId": "10c1df7f-9636-440a-b59d-86c689bd450d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg = \"Good Evening Everyone, Welcome to 'NLP' Class (AI)\""
      ],
      "metadata": {
        "id": "1l8TCLCDsvj-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9pPPa_2QtLlO",
        "outputId": "b9c5eda7-9b28-46ab-99f9-0731b2a6f5e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Good Evening Everyone, Welcome to 'NLP' Class (AI)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "pTLPxJbst1GP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in msg :\n",
        "  print(i)\n",
        "  time.sleep(0.5)  # prints each character (or item) in msg, one at a time, with a 0.5 second delay between each — creating a \"typing effect\"."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1CSJEuftdRm",
        "outputId": "681add04-d4e8-4d09-cb89-e4b69c73595e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\n",
            "o\n",
            "o\n",
            "d\n",
            " \n",
            "E\n",
            "v\n",
            "e\n",
            "n\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "E\n",
            "v\n",
            "e\n",
            "r\n",
            "y\n",
            "o\n",
            "n\n",
            "e\n",
            ",\n",
            " \n",
            "W\n",
            "e\n",
            "l\n",
            "c\n",
            "o\n",
            "m\n",
            "e\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "'\n",
            "N\n",
            "L\n",
            "P\n",
            "'\n",
            " \n",
            "C\n",
            "l\n",
            "a\n",
            "s\n",
            "s\n",
            " \n",
            "(\n",
            "A\n",
            "I\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eHO-JRGeuM51",
        "outputId": "216810b4-376a-48c2-ae1a-d0c37b214583"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5LqPAOBIuUjJ",
        "outputId": "89d0ecb8-20f4-40af-d9fb-b2cf7ba6a150"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Good Evening Everyone, Welcome to 'NLP' Class (AI)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in msg :\n",
        "   if i not in punc:\n",
        "      print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miUPpZFGtucr",
        "outputId": "3664071b-9df3-46a7-e5c5-9b836251ee62"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G\n",
            "o\n",
            "o\n",
            "d\n",
            " \n",
            "E\n",
            "v\n",
            "e\n",
            "n\n",
            "i\n",
            "n\n",
            "g\n",
            " \n",
            "E\n",
            "v\n",
            "e\n",
            "r\n",
            "y\n",
            "o\n",
            "n\n",
            "e\n",
            " \n",
            "W\n",
            "e\n",
            "l\n",
            "c\n",
            "o\n",
            "m\n",
            "e\n",
            " \n",
            "t\n",
            "o\n",
            " \n",
            "N\n",
            "L\n",
            "P\n",
            " \n",
            "C\n",
            "l\n",
            "a\n",
            "s\n",
            "s\n",
            " \n",
            "A\n",
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slist = []\n",
        "for i in msg :\n",
        "   if i not in punc:\n",
        "      slist.append(i)\n",
        "print(slist)\n",
        "print(\"\".join(slist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6teWf7guTeq",
        "outputId": "e4bf2795-46cb-43ad-8126-964041ad0790"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['G', 'o', 'o', 'd', ' ', 'E', 'v', 'e', 'n', 'i', 'n', 'g', ' ', 'E', 'v', 'e', 'r', 'y', 'o', 'n', 'e', ' ', 'W', 'e', 'l', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 'N', 'L', 'P', ' ', 'C', 'l', 'a', 's', 's', ' ', 'A', 'I']\n",
            "Good Evening Everyone Welcome to NLP Class AI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join([ c    for c in msg  if c not in punc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F7TqHDqduqrP",
        "outputId": "5bd9fbc4-c3b3-45f3-fdd4-28e20feaf713"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Good Evening Everyone Welcome to NLP Class AI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 70 25 June**"
      ],
      "metadata": {
        "id": "rhUbyCjtmPuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Special Character with Regular Expression**"
      ],
      "metadata": {
        "id": "Z95ZEX-8vTZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wish = \"Wish -You -A@ Happy_ New_ Year #2025!\""
      ],
      "metadata": {
        "id": "hHguVisDvFW6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wish"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XTNHKbD3xBJE",
        "outputId": "8d79ae1c-91a6-4a94-e3d0-bc5b7cf0b8c2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Wish -You -A@ Happy_ New_ Year #2025!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "r4_V0rDCxGcO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[a-z]\", wish)  # uses Python’s re module to find all lowercase alphabetic characters (a to z) in the string wish."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlAd5g2lxBpp",
        "outputId": "1459a922-7819-4656-ede9-12b1266becbb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 's', 'h', 'o', 'u', 'a', 'p', 'p', 'y', 'e', 'w', 'e', 'a', 'r']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re.findall(\"[a-zA-Z]\", wish)"
      ],
      "metadata": {
        "id": "Qq5a008bxj1-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[a-zA-Z0-9]\", wish)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L2IGAQwxoXp",
        "outputId": "6c9f54b4-26d2-4aeb-dbee-3805b9d9cfbb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['W',\n",
              " 'i',\n",
              " 's',\n",
              " 'h',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'u',\n",
              " 'A',\n",
              " 'H',\n",
              " 'a',\n",
              " 'p',\n",
              " 'p',\n",
              " 'y',\n",
              " 'N',\n",
              " 'e',\n",
              " 'w',\n",
              " 'Y',\n",
              " 'e',\n",
              " 'a',\n",
              " 'r',\n",
              " '2',\n",
              " '0',\n",
              " '2',\n",
              " '5']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"[^a-zA-Z0-9 ]\", wish)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jf3_uxvExyHN",
        "outputId": "3946fceb-c6e9-4fde-d84b-982bb02e7f90"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-', '-', '@', '_', '_', '#', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub(\"[^a-zA-Z0-9 ]\",\"\", wish)  #removes all characters from the string wish that are not letters, digits, or spaces."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IY_bI0nhx7aB",
        "outputId": "e20314e0-1fc7-48d7-95ac-9697d98af739"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Wish You A Happy New Year 2025'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re.findall()\t#Returns all matches in a list\n",
        "# re.search()\t#Returns first match (or None)\n",
        "# re.match()\t#Matches only from start of string\n",
        "# re.sub()\t#Substitutes matches with a new string\n",
        "# re.split()\t#Splits string based on a pattern\n",
        "# re.compile()\t#Precompiles a regex pattern for reuse"
      ],
      "metadata": {
        "id": "iVbNDx1uA16d"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Tokenization**\n",
        "\n",
        "* Tokenization is the process of breaking text into smaller pieces called tokens. These tokens can be:\n",
        " * Words, Subwords, Characters, or even punctuation marks\n",
        "* It’s a fundamental step in NLP used by models like GPT, BERT, etc.\n",
        "\n",
        "* nltk, spaCy, and HuggingFace tokenizers for both sentence and word tokenization"
      ],
      "metadata": {
        "id": "YjJQ-b20yb4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install nltk\n",
        "import nltk  #  Natural Language Toolkit\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('punkt')  #downloads the Punkt tokenizer models for the NLTK.This is required for functions like word_tokenize() to work properly.\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzTcVmdIyQG1",
        "outputId": "8d7fa743-c743-4705-d65e-b85b8586c489"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"Hello All! How are you doing today? We are implementing tokenization in NLP. This is done using NLTK.\"\n",
        "print(word_tokenize(para))\n",
        "# splits punctuation as separate tokens. Notice how !, ?, and . are treated as separate tokens\n",
        "# This is ideal for tasks like POS tagging, NER, or training models\n",
        "\n",
        "\n",
        "print(sent_tokenize(para)) #  keeps punctuation attached to sentence\n",
        "# Sentence boundaries are inferred based on punctuation and capitalization patterns"
      ],
      "metadata": {
        "id": "FOsXM0I3zu-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0550fe9e-49b4-408d-b227-7adeea6b611e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'All', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'We', 'are', 'implementing', 'tokenization', 'in', 'NLP', '.', 'This', 'is', 'done', 'using', 'NLTK', '.']\n",
            "['Hello All!', 'How are you doing today?', 'We are implementing tokenization in NLP.', 'This is done using NLTK.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(para.split())  #Splits text only on whitespace. Does not separate punctuation from words\n",
        "print(word_tokenize(para))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtnXL43l1-1A",
        "outputId": "c7ee7cdd-3c20-4454-fdaf-2195ef983991"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'All!', 'How', 'are', 'you', 'doing', 'today?', 'We', 'are', 'implementing', 'tokenization', 'in', 'NLP.', 'This', 'is', 'done', 'using', 'NLTK.']\n",
            "['Hello', 'All', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'We', 'are', 'implementing', 'tokenization', 'in', 'NLP', '.', 'This', 'is', 'done', 'using', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Stop Words**"
      ],
      "metadata": {
        "id": "d8puZ7ehIGJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Stop words are common words in a language (like \"the\", \"is\", \"in\", \"and\") that are usually filtered out before processing text in NLP, because they don’t carry much meaningful information. Doesn't contribute to semantic meaning of seuqential data\n",
        "\n",
        "* Why Remove Stop Words?\n",
        "   * They appear very frequently, but add little value to tasks like:\n",
        "    * Text classification\n",
        "    * Topic modeling\n",
        "    * Information retrieval\n",
        "   * Removing them reduces noise (focus on important words) and sometimes improves performance (by removing words, reduced memory usage)\n",
        "\n",
        "* when to remove stopwords-\n",
        "  * Text Classification -\n",
        "    * spam detection: if mail is having words like-\n",
        "      * Discount, offer, personal loans, excellent opportunity\n",
        "  * Search Engine- To ignore irrelevant terms in queries\n",
        "  * Document Clustering\n",
        "  * Sentiment Analysis\n",
        "  * Topic modeling (LDA): Stopwords add noise\n",
        "  * TF-IDF vectorization: Stopwords often have high frequency but low informativeness\n",
        "\n",
        "\n",
        "\n",
        "* When You Don’t Remove Them\n",
        "   * **Context-sensitive models (BERT, GPT)**: These models learn to assign low weight to stopwords, so removal isn't necessary.\n",
        "   * **Sequence tasks (e.g., translation, summarization)**: Removing stopwords would distort grammar and meaning."
      ],
      "metadata": {
        "id": "fu7CFqTvGK2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence: \"The cat is on the mat.\"\n",
        "\n",
        "# Without stopwords: \"cat mat\"\n",
        "\n",
        "# For a classifier, this may retain core meaning\n",
        "\n",
        "# For a translator or summarizer — this would break the sentence\n"
      ],
      "metadata": {
        "id": "2toSEx8b2hqp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  # imports the stop words corpus from the NLTK library, which provides predefined lists of common stop words in multiple languages (like English, French, German, etc.).\n",
        "nltk.download('stopwords') # Download the stopwords data (only once):\n",
        "print(stopwords.words('english'))\n",
        "print(len(stopwords.words('english')))"
      ],
      "metadata": {
        "id": "npW6qRCf2QNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc17cac-385e-4038-f60c-27c08b89a667"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
            "198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To list available languages like:\n",
        "print(stopwords.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJbvMKfgGjRX",
        "outputId": "a9cad974-91b7-4663-89c2-688af2de8434"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['albanian', 'arabic', 'azerbaijani', 'basque', 'belarusian', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'tamil', 'turkish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filtered\n",
        "print([word for word in word_tokenize(para) if word.lower() not in stopwords.words('english') and word not in string.punctuation])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZuVBar7HG7d",
        "outputId": "afdd251a-9df6-40f9-925f-74240864233c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'today', 'implementing', 'tokenization', 'NLP', 'done', 'using', 'NLTK']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Session 71 - 26 June**"
      ],
      "metadata": {
        "id": "2dsUXDmX0OjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = \"\"\"Defence Minister Rajnath Singh on Thursday has reportedly refused to sign the joint statement of the\n",
        "Shanghai Cooperation Organisation (SCO). As per reports, the defence minister refused to dot India's name on the\n",
        "document due to its failure to address India's concern regarding cross-border terrorism.\"\"\""
      ],
      "metadata": {
        "id": "5LlMUD8mHlS8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(news)"
      ],
      "metadata": {
        "id": "Hb2cc2n_0RDP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words:\n",
        "  if w not in stopwords.words('english'):\n",
        "    print(w,end=' ')\n",
        "    time.sleep(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifdhnSgVpFaA",
        "outputId": "86d85975-ef0d-4a60-dee1-b853ad655acf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defence Minister Rajnath Singh Thursday reportedly refused sign joint statement Shanghai Cooperation Organisation ( SCO ) . As per reports , defence minister refused dot India 's name document due failure address India 's concern regarding cross-border terrorism . "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join([w for w in words if w not in stopwords.words('english')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3_nIfxzypcFJ",
        "outputId": "0d28953f-cc54-4478-baa5-7a050fa3b825"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Defence Minister Rajnath Singh Thursday reportedly refused sign joint statement Shanghai Cooperation Organisation ( SCO ) . As per reports , defence minister refused dot India 's name document due failure address India 's concern regarding cross-border terrorism .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Stemming**"
      ],
      "metadata": {
        "id": "zDaY1dhxskRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
      ],
      "metadata": {
        "id": "ghnxVsXcqf9o"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps= PorterStemmer()  #One of the oldest and most common stemmers.Conservative: trims suffixes but preserves meaning better.\n",
        "ss=SnowballStemmer(language = \"english\") # More modern, multilingual, and slightly more aggressive than Porter. Often preferred over Porter for non-English or consistent stemming.\n",
        "ls=LancasterStemmer() #Very aggressive. May cut too much and produce root forms that aren't real words."
      ],
      "metadata": {
        "id": "pHofwlBOqzlN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem(\"running\")  # Output: run\n",
        "ps.stem(\"studies\")  # Output: studi\n",
        "ps.stem(\"happiness\")  # Output: happi\n",
        "ps.stem(\"maximum\")  # Output: maximum\n",
        "ps.stem(\"writing\") # write\n",
        "\n",
        "ss.stem(\"running\")  # Output: run\n",
        "ss.stem(\"studies\")  # Output: happi\n",
        "ss.stem(\"happiness\")  # Output: happy\n",
        "ss.stem(\"maximum\")  # Output: maximum\n",
        "ss.stem(\"writing\") # write\n",
        "\n",
        "ls.stem(\"running\")  # Output: run\n",
        "ls.stem(\"studies\")  # Output: study\n",
        "ls.stem(\"happiness\")  # Output: happy\n",
        "ls.stem(\"maximum\")  # Output: maxim\n",
        "ls.stem(\"writing\") # writ\n",
        "\n",
        "# Each stemmer is useful in different contexts:\n",
        "  # Use Porter for general English.\n",
        "  # Use Snowball for multilingual or slightly better balance.\n",
        "  # Use Lancaster when you need shorter roots but can tolerate distortion."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HjwkwGd62zBl",
        "outputId": "60b190ac-5c5c-44bb-a5ce-1a664c592452"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'writ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ps.stem(\"writing\"))\n",
        "print(ss.stem(\"writing\"))\n",
        "print(ls.stem(\"writing\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2L2etbbq-Ci",
        "outputId": "2d5c4109-4a37-4ef0-9f82-ce22a0c77c37"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "write\n",
            "write\n",
            "writ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inflected_words = [\"code\", \"coder\", \"coding\", \"coders\", \"codings\", \"change\", \"changes\", \"changing\", \"changed\",\n",
        "                   \"trouble\", \"troubled\",\"troubling\", \"troubles\", \"university\", \"universities\",\"universe\", \"universal\", \"run\",\n",
        "                   \"ran\", \"running\", \"runs\", \"writer\", \"write\", \"writers\", \"writes\", \"writing\"]\n",
        "print([ps.stem(i) for i in inflected_words])\n",
        "print([ss.stem(i) for i in inflected_words])\n",
        "print([ls.stem(i) for i in inflected_words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc9WOlCirWsS",
        "outputId": "c0eebfed-5a33-4929-cb2b-cd382e9bd545"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['code', 'coder', 'code', 'coder', 'code', 'chang', 'chang', 'chang', 'chang', 'troubl', 'troubl', 'troubl', 'troubl', 'univers', 'univers', 'univers', 'univers', 'run', 'ran', 'run', 'run', 'writer', 'write', 'writer', 'write', 'write']\n",
            "['code', 'coder', 'code', 'coder', 'code', 'chang', 'chang', 'chang', 'chang', 'troubl', 'troubl', 'troubl', 'troubl', 'univers', 'univers', 'univers', 'univers', 'run', 'ran', 'run', 'run', 'writer', 'write', 'writer', 'write', 'write']\n",
            "['cod', 'cod', 'cod', 'cod', 'cod', 'chang', 'chang', 'chang', 'chang', 'troubl', 'troubl', 'troubl', 'troubl', 'univers', 'univers', 'univers', 'univers', 'run', 'ran', 'run', 'run', 'writ', 'writ', 'writ', 'writ', 'writ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for  word in inflected_words:\n",
        "#   p = ps.stem(word)\n",
        "#   s = ss.stem(word)\n",
        "#   l = ls.stem(word)\n",
        "#   print(word,\"===>\",p,\"===>\",s,\"===>\", l)"
      ],
      "metadata": {
        "id": "CUAP-N6VsHYp"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Lemmatization**\n",
        "- Lemmatization is a NLP technique used to reduce words to their base or dictionary form, known as a lemma.\n",
        "- Unlike stemming, which often chops off word endings without understanding context, lemmatization uses vocabulary and grammar rules to return real words.\n",
        "- Lemmatizers consider the part of speech (POS) to determine the correct lemma. For example:\n",
        "  - running as a verb → run\n",
        "  - running as a noun (e.g., \"the running was fun\") → remains running"
      ],
      "metadata": {
        "id": "kZIU1GztuSIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "lemma = nltk.WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VmHvSj8sbBm",
        "outputId": "b7bb190a-b78b-4886-a3df-3ca009a31795"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma.lemmatize(\"writing\")\n",
        "# running\trun\n",
        "# better\tgood\n",
        "# studies\tstudy\n",
        "# was\tbe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u3naoFZMsyg-",
        "outputId": "cc37d325-4368-4a98-a2c7-17c0c4c58e74"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'writing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inflected_words = [\"code\", \"coder\", \"coding\", \"coders\", \"codings\", \"change\", \"changes\", \"changing\", \"changed\",\n",
        "                   \"trouble\", \"troubled\",\"troubling\", \"troubles\", \"university\", \"universities\",\"universe\", \"universal\", \"run\",\n",
        "                   \"ran\", \"running\", \"runs\", \"writer\", \"write\", \"writers\", \"writes\", \"writing\"]\n",
        "print([lemma.lemmatize(i) for i in inflected_words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okM3Oib_s86w",
        "outputId": "1e59a7fa-3247-49f5-ff3f-b6b5e0fecbaa"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['code', 'coder', 'coding', 'coder', 'coding', 'change', 'change', 'changing', 'changed', 'trouble', 'troubled', 'troubling', 'trouble', 'university', 'university', 'universe', 'universal', 'run', 'ran', 'running', 'run', 'writer', 'write', 'writer', 'writes', 'writing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for w in inflected_words:\n",
        "#   res = lemma.lemmatize(w)\n",
        "#   print(w,\" ===> \",res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HzXsvYFszB2",
        "outputId": "02e0e512-77ab-4b6c-f394-59a6531a0120"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "code  ===>  code\n",
            "coder  ===>  coder\n",
            "coding  ===>  coding\n",
            "coders  ===>  coder\n",
            "codings  ===>  coding\n",
            "change  ===>  change\n",
            "changes  ===>  change\n",
            "changing  ===>  changing\n",
            "changed  ===>  changed\n",
            "trouble  ===>  trouble\n",
            "troubled  ===>  troubled\n",
            "troubling  ===>  troubling\n",
            "troubles  ===>  trouble\n",
            "university  ===>  university\n",
            "universities  ===>  university\n",
            "universe  ===>  universe\n",
            "universal  ===>  universal\n",
            "run  ===>  run\n",
            "ran  ===>  ran\n",
            "running  ===>  running\n",
            "runs  ===>  run\n",
            "writer  ===>  writer\n",
            "write  ===>  write\n",
            "writers  ===>  writer\n",
            "writes  ===>  writes\n",
            "writing  ===>  writing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer  # The main lemmatizer class in NLTK.\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.corpus import wordnet  # provides part-of-speech (POS) constants (like wordnet.NOUN).\n",
        "from nltk import pos_tag, word_tokenize  # pos_tag tags each word in a sentence with its part of speech (e.g., noun, verb)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()  # Creates an instance of the lemmatizer.\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character for WordNetLemmatizer.\"\"\"\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {'J': wordnet.ADJ,\n",
        "                'N': wordnet.NOUN,\n",
        "                'V': wordnet.VERB,\n",
        "                'R': wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "word = \"writing\"\n",
        "lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "print(lemmatized_word)  # Output: run\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lan9VvOws2wS",
        "outputId": "a2746b65-5826-4a74-b2d0-4751033f77e1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "write\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news = \"\"\"Defence Minister Rajnath Singh on Thursday has reportedly refused to sign the joint statement of the\n",
        "Shanghai Cooperation Organisation (SCO). 123456 As per reports, the defence minister refused to dot India's name on the\n",
        "document due to its failure to address India's concern regarding cross-border terrorism.\"\"\""
      ],
      "metadata": {
        "id": "TLKfKsomvd1j"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "''.join([i for i in news if  i not in string.punctuation])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "S1fV3EeJwX8E",
        "outputId": "ced60900-08e6-4f8d-c6b5-319a6e376157"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Defence Minister Rajnath Singh on Thursday has reportedly refused to sign the joint statement of the\\nShanghai Cooperation Organisation SCO As per reports the defence minister refused to dot Indias name on the\\ndocument due to its failure to address Indias concern regarding crossborder terrorism'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# string.punctuation  #-> Contains all punctuation characters:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~. Used to remove punctuation during text cleaning.\n",
        "# re  #Python's built-in module for pattern matching and text manipulation. Common use: remove unwanted patterns like extra spaces, special characters, digits.\n",
        "# Tokenizations # Splits text into smaller units: words (word tokenization) or sentences (sentence tokenization).\n",
        "# Stopswords removal -> Stopwords are common words (e.g., “the”, “is”, “and”) that usually carry less meaning.\n",
        "# Stemming  -> Reduces words to their root form, which may not always be a valid word.\n",
        "# lemmatization -> Similar to stemming, but returns the dictionary base form (lemma) and uses POS tagging."
      ],
      "metadata": {
        "id": "pPnubO7ZwZ3D"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature              | `stopwords`                          | `string.punctuation`                  | `re` (Regular Expressions)                |                                            |\n",
        "| -------------------- | ------------------------------------ | ------------------------------------- | ----------------------------------------- | ------------------------------------------ |\n",
        "| **Module**           | `nltk.corpus.stopwords`              | Python `string` module                | Python `re` module                        |                                            |\n",
        "| **Purpose**          | Remove common, low-value words       | Remove punctuation/special characters | Remove or match patterns (flexible)       |                                            |\n",
        "| **Example Elements** | \"the\", \"is\", \"and\", \"in\", \"at\"       | `!\"#$%&'()*+,-./:;<=>?@[\\]^_`{        | }\\~\\`                                     | Any text pattern (e.g., `\\d+`, `[A-Za-z]`) |\n",
        "| **Use Case**         | Text simplification, keyword focus   | Cleaning symbols from text            | Cleaning, pattern matching, substitutions |                                            |\n",
        "| **Customizable?**    | Yes (you can add/remove words)       | No (fixed punctuation list)           | Yes (very flexible patterns)              |                                            |\n",
        "| **Example Code**     | `word in stopwords.words('english')` | `if char in string.punctuation`       | `re.sub(r'\\d+', '', text)`                |                                            |\n",
        "| **Returns**          | List of stop words                   | String of punctuation characters      | Modified string or match object           |                                            |\n",
        "\n",
        "-----------------------\n",
        "\n",
        "| Feature | **Stemming**                                | **Lemmatization**                                 |\n",
        "| ------- | ------------------------------------------- | ------------------------------------------------- |\n",
        "| Purpose | Reduces words to **root forms** by trimming | Reduces words to **dictionary base form** (lemma) |\n",
        "| Output  | May not be a real word                      | Always produces a valid word                      |\n",
        "| Method  | Rule-based, mechanical                      | Rule-based **+ vocabulary + grammar (POS)**       |\n",
        "\n",
        "| Scenario                                                   | Choose                                                |\n",
        "| ---------------------------------------------------------- | ----------------------------------------------------- |\n",
        "| Speed-sensitive pipeline                                   | Stemming                                              |\n",
        "| Meaning-preserving NLP task (like QA, chatbots, sentiment) | Lemmatization                                         |\n",
        "| Indexing/search engines                                    | Stemming (e.g., \"run\", \"runs\", \"running\" all → \"run\") |\n",
        "\n",
        "| Word             | **PorterStemmer** | **SnowballStemmer** | **LancasterStemmer** | **Lemmatization** |\n",
        "| ---------------- | ----------------- | ------------------- | -------------------- | ----------------- |\n",
        "| **running**      | run               | run                 | run                  | run               |\n",
        "| **studies**      | studi             | studi               | study                | study             |\n",
        "| **flies**        | fli               | fli                 | fly                  | fly               |\n",
        "| **happily**      | happili           | happili             | happy                | happily           |\n",
        "| **better**       | better            | better              | bet                  | good *(adj)*      |\n",
        "| **caring**       | care              | care                | car                  | care              |\n",
        "| **maximum**      | maximum           | maximum             | maxim                | maximum           |\n",
        "| **organization** | organ             | organ               | organ                | organization      |\n",
        "| **was**          | wa                | wa                  | was                  | be *(verb)*       |\n",
        "| **children**     | children          | children            | child                | child             |\n",
        "\n"
      ],
      "metadata": {
        "id": "yuEIBCzP4rEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# print(stopwords.words('english'))\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer\n",
        "ps= PorterStemmer()\n",
        "ss=SnowballStemmer(language = \"english\")\n",
        "ls=LancasterStemmer()\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemma = nltk.WordNetLemmatizer()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34M3ObBRxoo8",
        "outputId": "bd169cfd-36b5-4c8f-f91a-c4a516eff142"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join([lemma.lemmatize(ps.stem(i)) for i in word_tokenize (news) if i not in stopwords.words('english') and i not in string.punctuation])\n",
        "# ' '.join([ps.stem(i) for i in word_tokenize (news) if i not in stopwords.words('english') and i not in string.punctuation])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "70rs9TWgx0zJ",
        "outputId": "a5a01746-67aa-4202-8cbb-f8fa1ee34d52"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"defenc minist rajnath singh thursday reportedli refus sign joint statement shanghai cooper organis sco as per report defenc minist refus dot india 's name document due failur address india 's concern regard cross-bord terror\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join([lemma.lemmatize(i) for i in word_tokenize (news) if i not in stopwords.words('english') and i not in string.punctuation])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "zBgFR7FBy09e",
        "outputId": "10514106-ff6b-4d5a-a575-34afe6c8bbfb"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Defence Minister Rajnath Singh Thursday reportedly refused sign joint statement Shanghai Cooperation Organisation SCO As per report defence minister refused dot India 's name document due failure address India 's concern regarding cross-border terrorism\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# full pipeline example\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download required resources (only once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Input text\n",
        "text = \"Running faster than the wind, she was better at it than anyone!\"\n",
        "\n",
        "# Step 1: Lowercasing\n",
        "text = text.lower()\n",
        "\n",
        "# Step 2: Remove punctuation using regex\n",
        "text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Step 4: Stopword removal\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "# Step 5: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(w) for w in filtered_tokens]\n",
        "\n",
        "# Step 6: Lemmatization with POS\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    pos_dict = {'J': wordnet.ADJ, 'N': wordnet.NOUN,\n",
        "                'V': wordnet.VERB, 'R': wordnet.ADV}\n",
        "    return pos_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in filtered_tokens]\n",
        "\n",
        "# Output\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"After Stopword Removal:\", filtered_tokens)\n",
        "print(\"After Stemming:\", stemmed)\n",
        "print(\"After Lemmatization:\", lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL_26o_e0CvK",
        "outputId": "2b86e81e-e293-4d22-b4f8-728919e335bf"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['running', 'faster', 'than', 'the', 'wind', 'she', 'was', 'better', 'at', 'it', 'than', 'anyone']\n",
            "After Stopword Removal: ['running', 'faster', 'wind', 'better', 'anyone']\n",
            "After Stemming: ['run', 'faster', 'wind', 'better', 'anyon']\n",
            "After Lemmatization: ['run', 'faster', 'wind', 'well', 'anyone']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4RPvyU61OvH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}